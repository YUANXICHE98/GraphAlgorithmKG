# APIæ¥å£å’Œæ‰©å±•ç‚¹æ–‡æ¡£

## ğŸ¯ æ¥å£è®¾è®¡åŸåˆ™

**è¾“å…¥æ··ä¹±ï¼Œè¾“å‡ºè§„èŒƒ** - ç³»ç»Ÿæ¥å—ä»»æ„æ ¼å¼çš„è¾“å…¥ï¼Œä½†è¾“å‡ºå¿…é¡»ä¸¥æ ¼ç¬¦åˆSchemaè§„èŒƒ

## ğŸ”Œ æ ¸å¿ƒæ¥å£åˆ—è¡¨

### 1. ä¸»è¦å¤„ç†æ¥å£

#### ğŸ—ï¸ **çŸ¥è¯†å›¾è°±æ„å»ºæ¥å£**
```python
# æ–‡ä»¶: pipeline/schema_based_kg_builder.py
class SchemaBasedKGBuilder:
    def build_knowledge_graph(self, document_path: str, output_path: Optional[str] = None) -> KnowledgeGraph
```
- **åŠŸèƒ½**: å®Œæ•´çš„çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹
- **è¾“å…¥**: ä»»æ„æ ¼å¼æ–‡æ¡£è·¯å¾„
- **è¾“å‡º**: ç¬¦åˆSchemaçš„æ ‡å‡†åŒ–çŸ¥è¯†å›¾è°±
- **æ‰©å±•ç‚¹**: æ”¯æŒæ–°çš„æ–‡æ¡£æ ¼å¼è§£æå™¨

#### ğŸ” **Schemaæ£€æµ‹æ¥å£**
```python
# æ–‡ä»¶: ontology/managers/schema_detector.py
class SchemaDetector:
    def detect_schema(self, text: str, use_llm: bool = False) -> List[SchemaDetectionResult]
    def get_best_schema(self, text: str) -> Optional[str]
```
- **åŠŸèƒ½**: è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£é€‚ç”¨çš„Schemaç±»å‹
- **æ‰©å±•ç‚¹**: æ·»åŠ æ–°çš„Schemaç±»å‹å’Œæ£€æµ‹è§„åˆ™
- **LLMé›†æˆç‚¹**: `use_llm=True`æ—¶å¯ç”¨LLMè¾…åŠ©å†³ç­–

#### ğŸ”„ **æ··åˆä¸‰å…ƒç»„æŠ½å–æ¥å£**
```python
# æ–‡ä»¶: pipeline/hybrid_triple_extractor.py
class HybridTripleExtractor:
    def extract_triples(self, text: str) -> HybridExtractionResult
```
- **åŠŸèƒ½**: è§„åˆ™æŠ½å–ä¼˜å…ˆ + LLMå…œåº•çš„æ··åˆæŠ½å– (3é˜¶æ®µ: è§„åˆ™æŠ½å–â†’è´¨é‡è¯„ä¼°â†’LLMå…œåº•â†’ç»“æœåˆå¹¶)
- **æ‰©å±•ç‚¹**: æ·»åŠ æ–°çš„è§„åˆ™æ¨¡å¼å’ŒLLMç­–ç•¥
- **æ™ºèƒ½å†³ç­–**: è‡ªåŠ¨è¯„ä¼°è§„åˆ™æŠ½å–è´¨é‡ï¼Œå†³å®šæ˜¯å¦å¯ç”¨LLMå…œåº•

### 2. ç»„ä»¶æ¥å£

#### ğŸ§  **å®ä½“æ¨æ–­æ¥å£**
```python
# æ–‡ä»¶: pipeline/enhanced_entity_inferer.py
class EnhancedEntityTypeInferer:
    def infer_entity_type(self, entity_name: str) -> EntityInferenceResult
    def switch_domain(self, schema_path: str)
```
- **åŠŸèƒ½**: 6å±‚åˆ†å±‚å®ä½“ç±»å‹æ¨æ–­ (ç¼“å­˜â†’Schemaâ†’æ¨¡å¼â†’å…³é”®è¯â†’ä¸Šä¸‹æ–‡â†’Unknown)
- **æ‰©å±•ç‚¹**: æ·»åŠ æ–°çš„æ¨æ–­å±‚çº§å’Œç­–ç•¥
- **æ³¨æ„**: ç¬¬6å±‚æ ‡è®°ä¸ºUnknownï¼Œä¸ç›´æ¥è°ƒç”¨LLM

#### ğŸ¤– **LLMå®¢æˆ·ç«¯æ¥å£**
```python
# æ–‡ä»¶: pipeline/llm_client.py
class LLMClient:
    def generate_response(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str
    def extract_triples(self, text: str, schema_info: Dict[str, Any]) -> List[Dict[str, Any]]
    def validate_triple(self, subject: str, predicate: str, obj: str, schema_info: Dict[str, Any]) -> Dict[str, Any]
    def select_schema(self, text: str, candidate_schemas: List[Dict]) -> Dict
```
- **åŠŸèƒ½**: ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£ï¼Œæ”¯æŒé…ç½®æ–‡ä»¶å’Œç¯å¢ƒå˜é‡
- **æ‰©å±•ç‚¹**: æ”¯æŒå¤šç§LLMæœåŠ¡æä¾›å•†
- **é…ç½®**: è‡ªåŠ¨ä»config.jsonè¯»å–APIå¯†é’¥ã€Base URLç­‰é…ç½®
- **æ¨¡æ‹Ÿæ¨¡å¼**: å½“æœªé…ç½®APIæ—¶è‡ªåŠ¨å¯ç”¨æ¨¡æ‹Ÿæ¨¡å¼
- **ä½¿ç”¨åœºæ™¯**: Schemaé€‰æ‹©è¾…åŠ© + ä¸‰å…ƒç»„æŠ½å–å…œåº• + è¯­ä¹‰éªŒè¯è¾…åŠ©

#### ğŸ“ **ä¼šè¯ç®¡ç†æ¥å£**
```python
# æ–‡ä»¶: pipeline/session_manager.py
class SessionManager:
    def start_session(self, document_path: str) -> str
    def save_stage_result(self, stage_name: str, data: Any, description: str = "") -> str
    def save_final_result(self, kg_data: Dict[str, Any], analysis_data: Dict[str, Any] = None, schema_type: str = "mixed") -> Dict[str, str]
```
- **åŠŸèƒ½**: å®Œæ•´çš„ä¼šè¯ç”Ÿå‘½å‘¨æœŸç®¡ç†
- **æ‰©å±•ç‚¹**: æ·»åŠ æ–°çš„å­˜å‚¨åç«¯å’Œæ ¼å¼

### 3. æœ¬ä½“ç®¡ç†æ¥å£

#### ğŸ“‹ **åŠ¨æ€æœ¬ä½“ç®¡ç†æ¥å£**
```python
# æ–‡ä»¶: ontology/dynamic_schema.py
class DynamicOntologyManager:
    def load_schema(self, schema_path: str)
    def add_entity_type(self, entity_type: str, config: Dict[str, Any])
    def add_relation_type(self, relation_type: str, config: Dict[str, Any])
```
- **åŠŸèƒ½**: åŠ¨æ€åŠ è½½å’Œç®¡ç†æœ¬ä½“Schema
- **æ‰©å±•ç‚¹**: æ”¯æŒæ–°çš„Schemaæ ¼å¼å’ŒéªŒè¯è§„åˆ™

## ğŸ”§ æ‰©å±•ç‚¹è¯¦ç»†è¯´æ˜

### 1. **æ–°Schemaç±»å‹æ‰©å±•**

#### æ·»åŠ æ–°Schemaçš„æ­¥éª¤ï¼š
1. **åˆ›å»ºSchemaé…ç½®æ–‡ä»¶**
   ```yaml
   # æ–‡ä»¶: ontology/config/new_domain_schema.yaml
   metadata:
     name: "æ–°é¢†åŸŸæœ¬ä½“"
     version: "1.0.0"
     description: "æ–°é¢†åŸŸçš„çŸ¥è¯†å›¾è°±Schema"
   
   entity_types:
     NewEntityType:
       description: "æ–°å®ä½“ç±»å‹æè¿°"
       examples: ["ç¤ºä¾‹1", "ç¤ºä¾‹2"]
       keywords: ["å…³é”®è¯1", "å…³é”®è¯2"]
   
   relation_types:
     newRelationType:
       description: "æ–°å…³ç³»ç±»å‹æè¿°"
       subject_types: ["NewEntityType"]
       object_types: ["NewEntityType"]
   ```

2. **æ›´æ–°Schemaæ£€æµ‹å™¨**
   ```python
   # æ–‡ä»¶: ontology/schema_detector.py
   # åœ¨ _get_available_schemas() æ–¹æ³•ä¸­æ·»åŠ æ–°Schema
   schemas.append("new_domain_schema.yaml")
   ```

3. **æ·»åŠ åˆ†ç±»è§„åˆ™**
   ```python
   # æ–‡ä»¶: pipeline/session_manager.py
   # åœ¨ _classify_schema_type() æ–¹æ³•ä¸­æ·»åŠ åˆ†ç±»é€»è¾‘
   elif "new_domain" in schema_path_lower:
       return "new_domain"
   ```

### 2. **æ–°LLMæœåŠ¡å•†æ‰©å±•**

#### æ·»åŠ æ–°LLMæœåŠ¡çš„æ­¥éª¤ï¼š
1. **æ‰©å±•LLMå®¢æˆ·ç«¯**
   ```python
   # æ–‡ä»¶: pipeline/llm_client.py
   def _call_new_llm_api(self, prompt: str, max_tokens: int, temperature: float) -> str:
       """è°ƒç”¨æ–°LLMæœåŠ¡API"""
       # å®ç°æ–°çš„APIè°ƒç”¨é€»è¾‘
       pass
   ```

2. **æ›´æ–°é…ç½®**
   ```python
   # æ”¯æŒæ–°çš„ç¯å¢ƒå˜é‡
   NEW_LLM_API_KEY = os.getenv("NEW_LLM_API_KEY")
   NEW_LLM_BASE_URL = os.getenv("NEW_LLM_BASE_URL")
   ```

### 3. **æ–°æ–‡æ¡£æ ¼å¼æ‰©å±•**

#### æ·»åŠ æ–°æ–‡æ¡£æ ¼å¼æ”¯æŒï¼š
1. **æ‰©å±•æ–‡æ¡£åŠ è½½å™¨**
   ```python
   # æ–‡ä»¶: pipeline/schema_based_kg_builder.py
   def _load_document(self, document_path: str) -> str:
       # æ·»åŠ æ–°æ ¼å¼çš„è§£æé€»è¾‘
       if path.suffix.lower() == '.new_format':
           return self._parse_new_format(path)
   ```

### 4. **æ–°è§„åˆ™æ¨¡å¼æ‰©å±•**

#### æ·»åŠ æ–°çš„æŠ½å–è§„åˆ™ï¼š
1. **æ‰©å±•è§„åˆ™æŠ½å–å™¨**
   ```python
   # æ–‡ä»¶: pipeline/rule_based_triple_extractor.py
   def _compile_patterns(self):
       # æ·»åŠ æ–°çš„æ¨¡å¼ç±»åˆ«
       self.patterns['new_domain_patterns'] = [
           r'æ–°çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼1',
           r'æ–°çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼2'
       ]
   ```

## ğŸ¤– è®­ç»ƒæ¨¡å‹æ¥å£

### 1. **è®­ç»ƒæ¥å£åŸºç¡€æ¡†æ¶**

#### ğŸ—ï¸ **åŸºç¡€è®­ç»ƒå™¨æ¥å£**
```python
# æ–‡ä»¶: training/base_trainer.py
class BaseTrainer(ABC):
    def __init__(self, config: TrainingConfig)
    def prepare_data(self) -> Tuple[Any, Any, Any, Any]
    def build_model(self) -> Any
    def train_model(self, X_train, y_train, X_val, y_val) -> TrainingResult
    def evaluate_model(self, X_test, y_test) -> Dict[str, float]
    def run_full_training(self) -> TrainingResult
```
- **åŠŸèƒ½**: ç»Ÿä¸€çš„è®­ç»ƒæ¥å£å’Œæ¨¡å‹ç®¡ç†
- **æ‰©å±•ç‚¹**: ç»§æ‰¿æ­¤ç±»å®ç°ç‰¹å®šæ¨¡å‹çš„è®­ç»ƒå™¨

#### ğŸ“Š **è®­ç»ƒæ•°æ®ç®¡ç†æ¥å£**
```python
# æ–‡ä»¶: training/base_trainer.py
class TrainingDataManager:
    def collect_schema_training_data(self) -> str
    def collect_entity_training_data(self) -> str
    def collect_relation_training_data(self) -> str
```
- **åŠŸèƒ½**: ä»ä¼šè¯è®°å½•ä¸­è‡ªåŠ¨æ”¶é›†è®­ç»ƒæ•°æ®
- **æ‰©å±•ç‚¹**: æ·»åŠ æ–°çš„æ•°æ®æ”¶é›†ç­–ç•¥

### 2. **Schemaåˆ†ç±»æ¨¡å‹è®­ç»ƒæ¥å£**

#### ğŸ¯ **Schemaåˆ†ç±»å™¨è®­ç»ƒ**
```python
# æ–‡ä»¶: training/schema_classifier_trainer.py
class SchemaClassifierTrainer(BaseTrainer):
    def predict_schema(self, text: str) -> Dict[str, Any]

def train_schema_classifier(
    training_data_path: str = None,
    output_model_path: str = "training/models/schema_classifier.pkl",
    hyperparameters: Dict[str, Any] = None
) -> TrainingResult
```
- **åŠŸèƒ½**: è®­ç»ƒåŸºäºTF-IDFå’Œéšæœºæ£®æ—çš„Schemaåˆ†ç±»å™¨
- **è¾“å…¥**: æ–‡æ¡£æ–‡æœ¬å’Œå¯¹åº”çš„Schemaæ ‡ç­¾
- **è¾“å‡º**: è®­ç»ƒå¥½çš„åˆ†ç±»æ¨¡å‹
- **æ‰©å±•ç‚¹**: æ”¯æŒä¸åŒçš„ç‰¹å¾æå–å’Œåˆ†ç±»ç®—æ³•

#### ğŸ”§ **ä½¿ç”¨ç¤ºä¾‹**
```python
# è®­ç»ƒSchemaåˆ†ç±»å™¨
from training.schema_classifier_trainer import train_schema_classifier

result = train_schema_classifier(
    hyperparameters={
        'max_features': 5000,
        'n_estimators': 100,
        'max_depth': 10
    }
)

print(f"éªŒè¯å‡†ç¡®ç‡: {result.validation_accuracy:.4f}")
```

### 3. **å®ä½“æ¨æ–­æ¨¡å‹è®­ç»ƒæ¥å£**

#### ğŸ·ï¸ **å®ä½“æ¨æ–­å™¨è®­ç»ƒ**
```python
# æ–‡ä»¶: training/entity_inferer_trainer.py
class EntityInfererTrainer(BaseTrainer):
    def extract_features(self, entity_name: str, context: Dict[str, Any]) -> Dict[str, Any]
    def predict_entity_type(self, entity_name: str, context: Dict[str, Any]) -> Dict[str, Any]

def train_entity_inferer(
    training_data_path: str = None,
    output_model_path: str = "training/models/entity_inferer.pkl",
    hyperparameters: Dict[str, Any] = None
) -> TrainingResult
```
- **åŠŸèƒ½**: è®­ç»ƒåŸºäºç‰¹å¾å·¥ç¨‹å’Œæ¢¯åº¦æå‡çš„å®ä½“ç±»å‹æ¨æ–­å™¨
- **ç‰¹å¾**: æ–‡æœ¬ç‰¹å¾ã€è¯æ±‡ç‰¹å¾ã€æ¨¡å¼åŒ¹é…ã€é¢†åŸŸå…³é”®è¯
- **è¾“å‡º**: å®ä½“ç±»å‹å’Œç½®ä¿¡åº¦
- **æ‰©å±•ç‚¹**: æ·»åŠ æ–°çš„ç‰¹å¾æå–å™¨å’Œåˆ†ç±»ç®—æ³•

#### ğŸ”§ **ä½¿ç”¨ç¤ºä¾‹**
```python
# è®­ç»ƒå®ä½“æ¨æ–­å™¨
from training.entity_inferer_trainer import train_entity_inferer

result = train_entity_inferer(
    hyperparameters={
        'n_estimators': 100,
        'learning_rate': 0.1,
        'max_depth': 6
    }
)

print(f"éªŒè¯å‡†ç¡®ç‡: {result.validation_accuracy:.4f}")
```

### 4. **å…³ç³»æŠ½å–æ¨¡å‹è®­ç»ƒæ¥å£**

#### ğŸ”— **å…³ç³»æŠ½å–å™¨è®­ç»ƒ** (å¾…å®ç°)
```python
# æ–‡ä»¶: training/relation_extractor_trainer.py
class RelationExtractorTrainer(BaseTrainer):
    def extract_relation_features(self, subject: str, obj: str, context: str) -> Dict[str, Any]
    def predict_relation(self, subject: str, obj: str, context: str) -> Dict[str, Any]

def train_relation_extractor(
    training_data_path: str = None,
    output_model_path: str = "training/models/relation_extractor.pkl",
    hyperparameters: Dict[str, Any] = None
) -> TrainingResult
```
- **åŠŸèƒ½**: è®­ç»ƒå…³ç³»æŠ½å–å’Œåˆ†ç±»æ¨¡å‹
- **è¾“å…¥**: å®ä½“å¯¹å’Œä¸Šä¸‹æ–‡æ–‡æœ¬
- **è¾“å‡º**: å…³ç³»ç±»å‹å’Œç½®ä¿¡åº¦
- **æ‰©å±•ç‚¹**: æ”¯æŒä¸åŒçš„å…³ç³»æŠ½å–ç­–ç•¥

### 5. **å®ä½“åˆå¹¶æ¨¡å‹è®­ç»ƒæ¥å£**

#### ğŸ”„ **å®ä½“åˆå¹¶å™¨è®­ç»ƒ** (å¾…å®ç°)
```python
# æ–‡ä»¶: training/entity_merger_trainer.py
class EntityMergerTrainer(BaseTrainer):
    def calculate_entity_similarity(self, entity1: Dict, entity2: Dict) -> float
    def predict_merge_decision(self, entity1: Dict, entity2: Dict) -> Dict[str, Any]

def train_entity_merger(
    training_data_path: str = None,
    output_model_path: str = "training/models/entity_merger.pkl",
    hyperparameters: Dict[str, Any] = None
) -> TrainingResult
```
- **åŠŸèƒ½**: è®­ç»ƒå®ä½“å»é‡å’Œåˆå¹¶å†³ç­–æ¨¡å‹
- **è¾“å…¥**: å®ä½“å¯¹å’Œç›¸ä¼¼åº¦ç‰¹å¾
- **è¾“å‡º**: åˆå¹¶å†³ç­–å’Œç½®ä¿¡åº¦
- **æ‰©å±•ç‚¹**: æ”¯æŒä¸åŒçš„ç›¸ä¼¼åº¦è®¡ç®—å’Œå†³ç­–ç­–ç•¥

## ğŸš€ å¿«é€Ÿæ‰©å±•æŒ‡å—

### åœºæ™¯1: æ·»åŠ åŒ»ç–—é¢†åŸŸSchema
```bash
# 1. åˆ›å»ºSchemaæ–‡ä»¶
cp ontology/config/spatiotemporal_schema.yaml ontology/config/medical_schema.yaml

# 2. ä¿®æ”¹Schemaå†…å®¹ï¼ˆå®ä½“ç±»å‹ã€å…³ç³»ç±»å‹ï¼‰
# 3. æ›´æ–°æ£€æµ‹å™¨å’Œåˆ†ç±»å™¨
# 4. æµ‹è¯•æ–°Schema
python test_new_schema.py medical
```

### åœºæ™¯2: é›†æˆæ–°çš„LLMæœåŠ¡
```bash
# 1. å®‰è£…æ–°LLMçš„SDK
pip install new-llm-sdk

# 2. æ‰©å±•LLMå®¢æˆ·ç«¯
# 3. é…ç½®ç¯å¢ƒå˜é‡
export NEW_LLM_API_KEY="your-key"

# 4. æµ‹è¯•æ–°LLM
python test_llm_integration.py new-llm
```

### åœºæ™¯3: æ”¯æŒæ–°çš„æ–‡æ¡£æ ¼å¼
```bash
# 1. å®‰è£…è§£æåº“
pip install new-format-parser

# 2. æ‰©å±•æ–‡æ¡£åŠ è½½å™¨
# 3. æµ‹è¯•æ–°æ ¼å¼
python test_document_format.py new_format_file.ext
```

### åœºæ™¯4: è®­ç»ƒæ–°çš„Schemaåˆ†ç±»å™¨
```bash
# 1. æ”¶é›†è®­ç»ƒæ•°æ®
python -c "from training.base_trainer import training_data_manager; training_data_manager.collect_schema_training_data()"

# 2. è®­ç»ƒæ¨¡å‹
python -c "from training.schema_classifier_trainer import train_schema_classifier; train_schema_classifier()"

# 3. æµ‹è¯•æ–°æ¨¡å‹
python tests/unit/test_schema_system.py
```

### åœºæ™¯5: è®­ç»ƒå®ä½“æ¨æ–­å™¨
```bash
# 1. æ”¶é›†å®ä½“è®­ç»ƒæ•°æ®
python -c "from training.base_trainer import training_data_manager; training_data_manager.collect_entity_training_data()"

# 2. è®­ç»ƒæ¨¡å‹
python -c "from training.entity_inferer_trainer import train_entity_inferer; train_entity_inferer()"

# 3. è¯„ä¼°æ¨¡å‹æ€§èƒ½
python tests/unit/test_entity_inference.py
```

## ğŸ“Š æ¥å£æ€§èƒ½æŒ‡æ ‡

| æ¥å£ç±»å‹ | å“åº”æ—¶é—´ | ååé‡ | å‡†ç¡®ç‡ |
|----------|----------|--------|--------|
| Schemaæ£€æµ‹ | <100ms | >1000æ–‡æ¡£/å°æ—¶ | 100% |
| å®ä½“æ¨æ–­ | <2ms/å®ä½“ | >10000å®ä½“/å°æ—¶ | 80.4% |
| ä¸‰å…ƒç»„æŠ½å– | <50ms/æ–‡æ¡£ | >1000æ–‡æ¡£/å°æ—¶ | >75% |
| å…³ç³»éªŒè¯ | <1ms/å…³ç³» | >50000å…³ç³»/å°æ—¶ | 100% |
| ä¼šè¯ç®¡ç† | <10ms/æ“ä½œ | >10000æ“ä½œ/å°æ—¶ | 100% |
| æ¨¡å‹è®­ç»ƒ | 1-10åˆ†é’Ÿ | å–å†³äºæ•°æ®é‡ | >85% |

## ğŸ”’ æ¥å£å®‰å…¨å’ŒéªŒè¯

### è¾“å…¥éªŒè¯
- æ‰€æœ‰æ–‡æ¡£è·¯å¾„å¿…é¡»å­˜åœ¨ä¸”å¯è¯»
- Schemaæ–‡ä»¶å¿…é¡»ç¬¦åˆYAMLæ ¼å¼è§„èŒƒ
- å®ä½“å’Œå…³ç³»åç§°å¿…é¡»ç¬¦åˆå‘½åè§„èŒƒ

### è¾“å‡ºéªŒè¯
- æ‰€æœ‰å®ä½“ç±»å‹å¿…é¡»åœ¨Schemaä¸­å®šä¹‰
- æ‰€æœ‰å…³ç³»ç±»å‹å¿…é¡»åœ¨Schemaä¸­å®šä¹‰
- ä¸‰å…ƒç»„å¿…é¡»é€šè¿‡è¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥

### é”™è¯¯å¤„ç†
- ä¼˜é›…é™çº§ï¼šAPIå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼
- è¯¦ç»†æ—¥å¿—ï¼šè®°å½•æ‰€æœ‰å…³é”®æ“ä½œå’Œé”™è¯¯ä¿¡æ¯
- å¼‚å¸¸æ¢å¤ï¼šæ”¯æŒä»ä¸­é—´é˜¶æ®µæ¢å¤å¤„ç†

## ğŸ“ æ¥å£ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨
```python
from pipeline.schema_based_kg_builder import SchemaBasedKGBuilder

# åˆ›å»ºæ„å»ºå™¨
builder = SchemaBasedKGBuilder()

# æ„å»ºçŸ¥è¯†å›¾è°±
kg = builder.build_knowledge_graph("document.json")

# ç»“æœè‡ªåŠ¨ä¿å­˜åˆ°æŒ‰Schemaåˆ†ç±»çš„ç›®å½•
print(f"å®ä½“æ•°: {kg.statistics['total_entities']}")
print(f"å…³ç³»æ•°: {kg.statistics['total_relations']}")
```

### é«˜çº§ä½¿ç”¨
```python
from pipeline.session_manager import session_manager
from pipeline.hybrid_triple_extractor import HybridTripleExtractor

# æ‰‹åŠ¨ç®¡ç†ä¼šè¯
session_id = session_manager.start_session("document.json")

# ä½¿ç”¨æ··åˆæŠ½å–å™¨
extractor = HybridTripleExtractor(ontology_manager)
result = extractor.extract_triples(text)

# ä¿å­˜ä¸­é—´ç»“æœ
session_manager.save_stage_result("custom_extraction", result.triples)
```

## ğŸ¯ æ¥å£å‘å±•è·¯çº¿å›¾

### çŸ­æœŸç›®æ ‡ (1-2ä¸ªæœˆ)
- [ ] æ”¯æŒæ›´å¤šLLMæœåŠ¡å•† (Claude, Gemini, æœ¬åœ°æ¨¡å‹)
- [ ] æ·»åŠ åŒ»ç–—ã€é‡‘èã€æ³•å¾‹é¢†åŸŸSchema
- [ ] ä¼˜åŒ–å®ä½“æ¨æ–­å‡†ç¡®ç‡åˆ°85%+

### ä¸­æœŸç›®æ ‡ (3-6ä¸ªæœˆ)  
- [ ] æ”¯æŒå®æ—¶æµå¼å¤„ç†
- [ ] æ·»åŠ å›¾æ•°æ®åº“ç›´æ¥å¯¼å‡ºæ¥å£
- [ ] æ”¯æŒå¤šè¯­è¨€æ–‡æ¡£å¤„ç†

### é•¿æœŸç›®æ ‡ (6-12ä¸ªæœˆ)
- [ ] æ”¯æŒè‡ªåŠ¨Schemaå‘ç°å’Œç”Ÿæˆ
- [ ] é›†æˆçŸ¥è¯†å›¾è°±æ¨ç†å¼•æ“
- [ ] æ”¯æŒåˆ†å¸ƒå¼å¤„ç†å’Œé›†ç¾¤éƒ¨ç½²
