"""
å¤šçº§ç´¢å¼•ç³»ç»Ÿ
æ”¯æŒé«˜æ•ˆçš„å®ä½“ç±»å‹æ¨æ–­å’Œæ£€ç´¢
"""

import re
import time
from typing import Dict, List, Tuple, Optional, Set
from collections import defaultdict
from dataclasses import dataclass
import pickle
import json
from pathlib import Path

@dataclass
class IndexEntry:
    """ç´¢å¼•æ¡ç›®"""
    term: str
    entity_type: str
    confidence: float
    source: str  # 'keyword', 'pattern', 'learned'

class InvertedIndex:
    """å€’æ’ç´¢å¼•ï¼šå…³é”®è¯ -> å®ä½“ç±»å‹"""
    
    def __init__(self):
        self.index: Dict[str, List[IndexEntry]] = defaultdict(list)
    
    def add(self, term: str, entity_type: str, confidence: float = 1.0, source: str = 'keyword'):
        """æ·»åŠ ç´¢å¼•é¡¹"""
        term_lower = term.lower()
        entry = IndexEntry(term, entity_type, confidence, source)
        
        # é¿å…é‡å¤æ·»åŠ ç›¸åŒçš„æ¡ç›®
        existing = [e for e in self.index[term_lower] if e.entity_type == entity_type and e.source == source]
        if not existing:
            self.index[term_lower].append(entry)
    
    def get(self, term: str) -> List[IndexEntry]:
        """è·å–åŒ¹é…çš„ç´¢å¼•é¡¹"""
        return self.index.get(term.lower(), [])
    
    def search_partial(self, term: str) -> List[IndexEntry]:
        """éƒ¨åˆ†åŒ¹é…æœç´¢"""
        term_lower = term.lower()
        results = []
        
        for indexed_term, entries in self.index.items():
            if term_lower in indexed_term:
                results.extend(entries)
        
        return results

class TrieNode:
    """å‰ç¼€æ ‘èŠ‚ç‚¹"""
    
    def __init__(self):
        self.children: Dict[str, 'TrieNode'] = {}
        self.entries: List[IndexEntry] = []
        self.is_end = False

class TrieIndex:
    """å‰ç¼€æ ‘ç´¢å¼•ï¼šå¿«é€Ÿå‰ç¼€åŒ¹é…"""
    
    def __init__(self):
        self.root = TrieNode()
    
    def insert(self, term: str, entity_type: str, confidence: float = 0.8, source: str = 'pattern'):
        """æ’å…¥è¯æ¡"""
        node = self.root
        term_lower = term.lower()
        
        for char in term_lower:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        
        node.is_end = True
        entry = IndexEntry(term, entity_type, confidence, source)
        node.entries.append(entry)
    
    def search_prefix(self, prefix: str, max_results: int = 10) -> List[IndexEntry]:
        """å‰ç¼€æœç´¢"""
        node = self.root
        prefix_lower = prefix.lower()
        
        # æ‰¾åˆ°å‰ç¼€å¯¹åº”çš„èŠ‚ç‚¹
        for char in prefix_lower:
            if char not in node.children:
                return []
            node = node.children[char]
        
        # æ”¶é›†æ‰€æœ‰ä»¥è¯¥å‰ç¼€å¼€å¤´çš„è¯æ¡
        results = []
        self._collect_entries(node, results, max_results)
        return results
    
    def _collect_entries(self, node: TrieNode, results: List[IndexEntry], max_results: int):
        """é€’å½’æ”¶é›†æ¡ç›®"""
        if len(results) >= max_results:
            return
        
        if node.is_end:
            results.extend(node.entries)
        
        for child in node.children.values():
            self._collect_entries(child, results, max_results)

class PatternIndex:
    """æ¨¡å¼ç´¢å¼•ï¼šæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…"""
    
    def __init__(self):
        self.patterns: List[Tuple[str, str, float]] = []  # (pattern, entity_type, confidence)
    
    def add_pattern(self, pattern: str, entity_type: str, confidence: float = 0.8):
        """æ·»åŠ æ¨¡å¼"""
        try:
            # éªŒè¯æ­£åˆ™è¡¨è¾¾å¼
            re.compile(pattern)
            self.patterns.append((pattern, entity_type, confidence))
        except re.error as e:
            print(f"âš ï¸ æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼ {pattern}: {e}")
    
    def match(self, term: str) -> List[IndexEntry]:
        """æ¨¡å¼åŒ¹é…"""
        results = []
        
        for pattern, entity_type, confidence in self.patterns:
            try:
                if re.match(pattern, term, re.IGNORECASE):
                    entry = IndexEntry(term, entity_type, confidence, 'pattern')
                    results.append(entry)
            except re.error:
                continue
        
        return results

class MultiLevelIndex:
    """å¤šçº§ç´¢å¼•ç³»ç»Ÿ"""
    
    def __init__(self, index_dir: str = "indexing/data"):
        self.index_dir = Path(index_dir)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¬¬1çº§ï¼šå€’æ’ç´¢å¼•ï¼ˆå…³é”®è¯ -> å®ä½“ç±»å‹ï¼‰
        self.inverted_index = InvertedIndex()
        
        # ç¬¬2çº§ï¼šå‰ç¼€æ ‘ï¼ˆå¿«é€Ÿå‰ç¼€åŒ¹é…ï¼‰
        self.trie_index = TrieIndex()
        
        # ç¬¬3çº§ï¼šæ¨¡å¼ç´¢å¼•ï¼ˆæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼‰
        self.pattern_index = PatternIndex()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_terms': 0,
            'total_patterns': 0,
            'last_updated': time.time()
        }
    
    def build_indexes(self, domain_vocabulary: Dict[str, Dict[str, List[str]]]):
        """æ„å»ºæ‰€æœ‰ç´¢å¼•"""
        print("ğŸ”¨ æ„å»ºå¤šçº§ç´¢å¼•...")
        
        for domain_name, vocabulary in domain_vocabulary.items():
            print(f"  ğŸ“š å¤„ç†é¢†åŸŸ: {domain_name}")
            
            # æ„å»ºå…³é”®è¯ç´¢å¼•
            if 'keywords' in vocabulary:
                for entity_type, keywords in vocabulary['keywords'].items():
                    for keyword in keywords:
                        self.inverted_index.add(keyword, entity_type, 1.0, 'keyword')
                        self.trie_index.insert(keyword, entity_type, 0.9, 'keyword')
                        self.stats['total_terms'] += 1
            
            # æ„å»ºæ¨¡å¼ç´¢å¼•
            if 'patterns' in vocabulary:
                for entity_type, patterns in vocabulary['patterns'].items():
                    for pattern in patterns:
                        self.pattern_index.add_pattern(pattern, entity_type, 0.8)
                        self.stats['total_patterns'] += 1
        
        self.stats['last_updated'] = time.time()
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {self.stats['total_terms']} ä¸ªè¯æ¡, {self.stats['total_patterns']} ä¸ªæ¨¡å¼")
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """å¤šçº§æ£€ç´¢"""
        results = []
        query_lower = query.lower()
        
        # ç¬¬1çº§ï¼šç²¾ç¡®åŒ¹é…
        exact_matches = self.inverted_index.get(query)
        for entry in exact_matches:
            results.append((entry.entity_type, entry.confidence))
        
        # ç¬¬2çº§ï¼šå‰ç¼€åŒ¹é…
        if len(results) < top_k:
            prefix_matches = self.trie_index.search_prefix(query, top_k - len(results))
            for entry in prefix_matches:
                results.append((entry.entity_type, entry.confidence * 0.9))  # ç¨å¾®é™ä½ç½®ä¿¡åº¦
        
        # ç¬¬3çº§ï¼šæ¨¡å¼åŒ¹é…
        if len(results) < top_k:
            pattern_matches = self.pattern_index.match(query)
            for entry in pattern_matches:
                results.append((entry.entity_type, entry.confidence))
        
        # ç¬¬4çº§ï¼šéƒ¨åˆ†åŒ¹é…
        if len(results) < top_k:
            partial_matches = self.inverted_index.search_partial(query)
            for entry in partial_matches:
                results.append((entry.entity_type, entry.confidence * 0.7))  # è¿›ä¸€æ­¥é™ä½ç½®ä¿¡åº¦
        
        return self._deduplicate_and_rank(results, top_k)
    
    def _deduplicate_and_rank(self, results: List[Tuple[str, float]], top_k: int) -> List[Tuple[str, float]]:
        """å»é‡å¹¶æ’åº"""
        # æŒ‰å®ä½“ç±»å‹åˆ†ç»„ï¼Œå–æœ€é«˜ç½®ä¿¡åº¦
        type_scores = {}
        for entity_type, confidence in results:
            if entity_type not in type_scores or confidence > type_scores[entity_type]:
                type_scores[entity_type] = confidence
        
        # æ’åºå¹¶è¿”å›top_k
        sorted_results = sorted(type_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_results[:top_k]
    
    def add_learned_term(self, term: str, entity_type: str, confidence: float, source: str = 'learned'):
        """æ·»åŠ å­¦ä¹ åˆ°çš„è¯æ¡"""
        self.inverted_index.add(term, entity_type, confidence, source)
        self.trie_index.insert(term, entity_type, confidence, source)
        self.stats['total_terms'] += 1
    
    def batch_update(self, updates: List[Dict]):
        """æ‰¹é‡æ›´æ–°ç´¢å¼•"""
        for update in updates:
            if update['action'] == 'add':
                self.add_learned_term(
                    update['term'], 
                    update['entity_type'], 
                    update.get('confidence', 0.8),
                    update.get('source', 'learned')
                )
        
        self.stats['last_updated'] = time.time()
        print(f"ğŸ”„ æ‰¹é‡æ›´æ–°å®Œæˆ: {len(updates)} é¡¹")
    
    def save_indexes(self):
        """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜"""
        try:
            # ä¿å­˜å€’æ’ç´¢å¼•
            with open(self.index_dir / 'inverted_index.pkl', 'wb') as f:
                pickle.dump(self.inverted_index.index, f)
            
            # ä¿å­˜å‰ç¼€æ ‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
            trie_data = self._serialize_trie()
            with open(self.index_dir / 'trie_index.json', 'w', encoding='utf-8') as f:
                json.dump(trie_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æ¨¡å¼ç´¢å¼•
            with open(self.index_dir / 'pattern_index.json', 'w', encoding='utf-8') as f:
                json.dump(self.pattern_index.patterns, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            with open(self.index_dir / 'stats.json', 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜åˆ°: {self.index_dir}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
    
    def load_indexes(self):
        """ä»ç£ç›˜åŠ è½½ç´¢å¼•"""
        try:
            # åŠ è½½å€’æ’ç´¢å¼•
            inverted_file = self.index_dir / 'inverted_index.pkl'
            if inverted_file.exists():
                with open(inverted_file, 'rb') as f:
                    self.inverted_index.index = pickle.load(f)
            
            # åŠ è½½æ¨¡å¼ç´¢å¼•
            pattern_file = self.index_dir / 'pattern_index.json'
            if pattern_file.exists():
                with open(pattern_file, 'r', encoding='utf-8') as f:
                    self.pattern_index.patterns = json.load(f)
            
            # åŠ è½½ç»Ÿè®¡ä¿¡æ¯
            stats_file = self.index_dir / 'stats.json'
            if stats_file.exists():
                with open(stats_file, 'r', encoding='utf-8') as f:
                    self.stats = json.load(f)
            
            print(f"ğŸ“‚ ç´¢å¼•å·²åŠ è½½: {self.stats.get('total_terms', 0)} ä¸ªè¯æ¡")
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
    
    def _serialize_trie(self) -> Dict:
        """åºåˆ—åŒ–å‰ç¼€æ ‘ï¼ˆç®€åŒ–ç‰ˆï¼Œåªä¿å­˜å¶å­èŠ‚ç‚¹ï¼‰"""
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œåªä¿å­˜å®Œæ•´è¯æ¡
        # å®é™…åº”ç”¨ä¸­å¯ä»¥å®ç°å®Œæ•´çš„å‰ç¼€æ ‘åºåˆ—åŒ–
        return {"note": "Trie serialization simplified"}
    
    def get_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            'inverted_index_size': len(self.inverted_index.index),
            'pattern_count': len(self.pattern_index.patterns)
        }

# å…¨å±€å¤šçº§ç´¢å¼•å®ä¾‹
multi_level_index = MultiLevelIndex()
