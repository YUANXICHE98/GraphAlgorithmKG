"""
çŸ¥è¯†å›¾è°±æ„å»ºç³»ç»Ÿ - ç»Ÿä¸€ä¸»ç¨‹åº
æ•´åˆæ–‡æ¡£å¤„ç†ã€LLMæŠ½å–ã€åŠ¨æ€æœ¬ä½“æ‰©å±•ã€Neo4jé›†æˆçš„å®Œæ•´æµç¨‹
"""

import argparse
import json
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# æ ¸å¿ƒæ¨¡å—
from ontology.dynamic_schema import DynamicOntologyManager
from pipeline.document_processor import DocumentProcessor
from pipeline.text_splitter import TextSplitter
from pipeline.llm_extractor import LLMKGExtractor
from pipeline.schema_reviewer import SchemaReviewer
from pipeline.dynamic_mapper import DynamicOntologyMapper
from pipeline.kg_updater import KnowledgeGraphUpdater
from pipeline.kg_retriever import KGRetriever
from pipeline.neo4j_connector import import_storage_to_neo4j, export_neo4j_to_files
from pipeline.triple_cleaner import TripleCleaner

class KnowledgeGraphBuilder:
    """ç»Ÿä¸€çŸ¥è¯†å›¾è°±æ„å»ºå™¨"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._load_default_config()
        
        # ç»Ÿä¸€ä½¿ç”¨åŠ¨æ€å­˜å‚¨è·¯å¾„
        storage_path = self.config.get('storage_path', 'dynamic_kg_storage')
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.ontology = DynamicOntologyManager()
        self.document_processor = DocumentProcessor()
        self.text_splitter = TextSplitter(
            max_tokens=self.config.get('chunk_size', 2000),
            overlap=self.config.get('chunk_overlap', 200)
        )
        self.llm_extractor = LLMKGExtractor(self.ontology)
        self.schema_reviewer = SchemaReviewer()
        self.triple_cleaner = TripleCleaner()
        self.mapper = DynamicOntologyMapper(self.ontology)
        self.kg_updater = KnowledgeGraphUpdater(self.ontology, storage_path)
        self.retriever = KGRetriever(self.ontology, storage_path)
        
        # ä¼šè¯ç»Ÿè®¡
        self.session_stats = {
            "start_time": datetime.now().isoformat(),
            "documents_processed": 0,
            "chunks_extracted": 0,
            "triples_generated": 0,
            "entities_created": 0,
            "relations_created": 0,
            "ontology_updates": 0
        }

    def process_documents(self, input_paths: List[str], **kwargs) -> Dict[str, Any]:
        """å¤„ç†æ–‡æ¡£çš„å®Œæ•´æµç¨‹"""
        print("ğŸš€ å¼€å§‹æ–‡æ¡£å¤„ç†æµç¨‹...")
        
        # æ–‡æ¡£å¤„ç† -> æ–‡æœ¬åˆ‡ç‰‡ -> LLMæŠ½å– -> æœ¬ä½“æ˜ å°„ -> å›¾è°±æ›´æ–°
        all_triples = []
        
        for path in input_paths:
            print(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {path}")
            
            # 1. æ–‡æ¡£å¤„ç†
            doc = self.document_processor.process_document(path)
            self.session_stats["documents_processed"] += 1
            
            # 2. æ–‡æœ¬åˆ‡ç‰‡
            chunks = self.text_splitter.split_text(doc.content)
            self.session_stats["chunks_extracted"] += len(chunks)
            
            # 3. LLMæŠ½å–
            for chunk in chunks:
                extraction_result = self.llm_extractor.extract_from_text(chunk)
                if extraction_result.success:
                    all_triples.extend(extraction_result.triples)
                else:
                    print(f"è­¦å‘Š: æ–‡æœ¬å—æŠ½å–å¤±è´¥: {extraction_result.error_message}")
        
        return self._process_triples_pipeline(all_triples, **kwargs)

    def process_triples_file(self, file_path: str, **kwargs) -> Dict[str, Any]:
        """å¤„ç†ä¸‰å…ƒç»„æ–‡ä»¶"""
        print(f"ğŸ“‚ åŠ è½½ä¸‰å…ƒç»„æ–‡ä»¶: {file_path}")
        
        triples = self._load_triples_from_file(file_path)
        return self._process_triples_pipeline(triples, **kwargs)

    def _process_triples_pipeline(self, triples: List[Dict], **kwargs) -> Dict[str, Any]:
        """ä¸‰å…ƒç»„å¤„ç†æµæ°´çº¿"""
        if not triples:
            print("âš ï¸  æ²¡æœ‰ä¸‰å…ƒç»„éœ€è¦å¤„ç†")
            return {"status": "no_data"}
        
        print(f"ğŸ”„ å¼€å§‹å¤„ç† {len(triples)} ä¸ªä¸‰å…ƒç»„...")
        
        # 1. æ¸…ç†ä¸‰å…ƒç»„
        cleaned_triples = self.triple_cleaner.clean_triples(triples)
        print(f"   âœ… æ¸…ç†å: {len(cleaned_triples)} ä¸ªä¸‰å…ƒç»„")
        
        # 2. æœ¬ä½“æ˜ å°„ï¼ˆåŒ…å«åŠ¨æ€æ‰©å±•ï¼‰
        mapping_results = self.mapper.map_triples(cleaned_triples)
        mapped_triples = [r.mapped_triple for r in mapping_results if r.success]
        print(f"   âœ… æ˜ å°„æˆåŠŸ: {len(mapped_triples)} ä¸ªä¸‰å…ƒç»„")
        
        # 3. æ›´æ–°çŸ¥è¯†å›¾è°±
        update_result = self.kg_updater.update_kg_from_triples(mapped_triples)
        
        self.session_stats["triples_generated"] = len(triples)
        self.session_stats["entities_created"] = update_result.new_entities
        self.session_stats["relations_created"] = update_result.new_relations
        
        print(f"   âœ… æ–°å¢å®ä½“: {update_result.new_entities}")
        print(f"   âœ… æ–°å¢å…³ç³»: {update_result.new_relations}")
        
        print("\nğŸ‰ çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ!")
        self._print_session_summary()
        
        return {
            "status": "completed",
            "stats": self.session_stats,
            "ontology_version": self.ontology.current_version,
            "storage_path": self.config.get('storage_path', 'dynamic_kg_storage')
        }

    def search_knowledge_graph(self, query: str, max_hops: int = 2) -> Dict[str, Any]:
        """æœç´¢çŸ¥è¯†å›¾è°±"""
        search_result = self.retriever.retrieve_subgraph(query, hops=max_hops)
        return {
            'nodes': search_result.subgraph.get('nodes', []),
            'edges': search_result.subgraph.get('edges', []),
            'matched_entities': search_result.matched_entities,
            'search_time': search_result.search_time,
            'total_nodes': search_result.total_nodes,
            'total_edges': search_result.total_edges
        }

    def export_to_neo4j(self, clear_existing: bool = False, **neo4j_config) -> bool:
        """å¯¼å‡ºåˆ°Neo4jæ•°æ®åº“"""
        config = self.config.get('neo4j', {})
        config.update(neo4j_config)
        
        return import_storage_to_neo4j(
            storage_path=self.config.get('storage_path', 'dynamic_kg_storage'),
            uri=config.get('uri', 'bolt://localhost:7687'),
            user=config.get('user', 'neo4j'),
            password=config.get('password', 'yuanxi98'),
            clear_existing=clear_existing
        )

    def _load_triples_from_file(self, file_path: str) -> List[Dict]:
        """ä»æ–‡ä»¶åŠ è½½ä¸‰å…ƒç»„"""
        file_path = Path(file_path)
        
        if file_path.suffix.lower() == '.csv':
            import pandas as pd
            df = pd.read_csv(file_path)
            return [
                {"subject": row['subject'], "predicate": row['predicate'], "object": row['object']}
                for _, row in df.iterrows()
            ]
        elif file_path.suffix.lower() == '.json':
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")

    def _load_default_config(self) -> Dict:
        """åŠ è½½é»˜è®¤é…ç½®"""
        config_file = Path("config.json")
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        return {
            'chunk_size': 2000,
            'chunk_overlap': 200,
            'storage_path': 'dynamic_kg_storage',
            'llm_model': 'gpt-3.5-turbo',
            'max_tokens': 4000,
            'temperature': 0.1
        }

    def _print_session_summary(self):
        """æ‰“å°ä¼šè¯æ‘˜è¦"""
        print(f"\nğŸ“Š å¤„ç†æ‘˜è¦:")
        print(f"   æ–‡æ¡£æ•°é‡: {self.session_stats['documents_processed']}")
        print(f"   æ–‡æœ¬å—æ•°: {self.session_stats['chunks_extracted']}")
        print(f"   ä¸‰å…ƒç»„æ•°: {self.session_stats['triples_generated']}")
        print(f"   æ–°å¢å®ä½“: {self.session_stats['entities_created']}")
        print(f"   æ–°å¢å…³ç³»: {self.session_stats['relations_created']}")
        print(f"   æœ¬ä½“ç‰ˆæœ¬: {self.ontology.current_version}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="çŸ¥è¯†å›¾è°±æ„å»ºç³»ç»Ÿ")
    
    # è¾“å…¥å‚æ•°
    parser.add_argument("input_paths", nargs="*", help="è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--storage-path", default="dynamic_kg_storage", help="å­˜å‚¨è·¯å¾„")
    
    # å¤„ç†é€‰é¡¹
    parser.add_argument("--no-review", action="store_true", help="è·³è¿‡äººå·¥å¤æ ¸")
    parser.add_argument("--auto-approve", action="store_true", help="è‡ªåŠ¨æ‰¹å‡†æ‰€æœ‰æ›´æ–°")
    
    # æœç´¢é€‰é¡¹
    parser.add_argument("--search", help="æœç´¢çŸ¥è¯†å›¾è°±")
    parser.add_argument("--search-hops", type=int, default=2, help="æœç´¢è·³æ•°")
    
    # Neo4jé€‰é¡¹
    parser.add_argument("--export-neo4j", action="store_true", help="å¯¼å‡ºåˆ°Neo4j")
    parser.add_argument("--neo4j-clear", action="store_true", help="æ¸…ç©ºNeo4jæ•°æ®åº“")
    parser.add_argument("--neo4j-uri", default="bolt://localhost:7687", help="Neo4j URI")
    parser.add_argument("--neo4j-user", default="neo4j", help="Neo4jç”¨æˆ·å")
    parser.add_argument("--neo4j-password", default="yuanxi98", help="Neo4jå¯†ç ")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = None
    if args.config:
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
    
    # æ›´æ–°å­˜å‚¨è·¯å¾„
    if config:
        config['storage_path'] = args.storage_path
    else:
        config = {'storage_path': args.storage_path}
    
    # åˆ›å»ºæ„å»ºå™¨
    builder = KnowledgeGraphBuilder(config)
    
    try:
        # æœç´¢æ¨¡å¼
        if args.search:
            print(f"ğŸ” æœç´¢çŸ¥è¯†å›¾è°±: {args.search}")
            results = builder.search_knowledge_graph(args.search, args.search_hops)
            print(json.dumps(results, ensure_ascii=False, indent=2))
            return
        
        # Neo4jå¯¼å‡ºæ¨¡å¼
        if args.export_neo4j:
            if args.input_paths:
                # å…ˆå¤„ç†æ•°æ®å†å¯¼å‡º
                if args.input_paths[0].endswith(('.csv', '.json')):
                    builder.process_triples_file(args.input_paths[0])
                else:
                    builder.process_documents(args.input_paths)
            
            # å¯¼å‡ºåˆ°Neo4j
            print("ğŸ“¤ å¯¼å‡ºåˆ°Neo4jæ•°æ®åº“...")
            success = builder.export_to_neo4j(
                clear_existing=args.neo4j_clear,
                uri=args.neo4j_uri,
                user=args.neo4j_user,
                password=args.neo4j_password
            )
            if success:
                print(f"ğŸŒ Neo4jæµè§ˆå™¨: http://localhost:7474")
            return
        
        # æ ‡å‡†å¤„ç†æ¨¡å¼
        if not args.input_paths:
            print("âŒ è¯·æä¾›è¾“å…¥æ–‡ä»¶æˆ–æŒ‡å®šæ“ä½œ")
            parser.print_help()
            return
        
        # åˆ¤æ–­è¾“å…¥ç±»å‹å¹¶å¤„ç†
        if args.input_paths[0].endswith(('.csv', '.json')):
            # ä¸‰å…ƒç»„æ–‡ä»¶
            for file_path in args.input_paths:
                builder.process_triples_file(file_path)
        else:
            # æ–‡æ¡£æ–‡ä»¶
            builder.process_documents(args.input_paths)
            
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
