"""
å¢å¼ºçš„å®ä½“ç±»å‹æ¨æ–­å™¨
åŸºäºåˆ†å±‚åŒ¹é…ç­–ç•¥çš„å®ä½“ç±»å‹æ¨æ–­ç³»ç»Ÿ
"""

import re
import time
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from dataclasses import dataclass

@dataclass
class InferenceResult:
    """æ¨æ–­ç»“æœ"""
    entity_type: Optional[str]
    confidence: float
    method: str  # 'cache', 'pattern', 'keyword', 'context', 'unknown'
    inference_time: float

class EntityTypeCache:
    """å®ä½“ç±»å‹ç¼“å­˜"""
    
    def __init__(self):
        self.verified_entities = {}
        self.confidence_threshold = 0.9
    
    def get(self, entity_name: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„å®ä½“ç±»å‹"""
        return self.verified_entities.get(entity_name.lower())
    
    def add_verified_entity(self, entity_name: str, entity_type: str, 
                          confidence: float, source: str):
        """æ·»åŠ å·²éªŒè¯çš„å®ä½“ç±»å‹"""
        key = entity_name.lower()
        if key in self.verified_entities:
            # æ›´æ–°ä½¿ç”¨è®¡æ•°
            self.verified_entities[key]['count'] += 1
            self.verified_entities[key]['last_used'] = time.time()
        else:
            self.verified_entities[key] = {
                'type': entity_type,
                'confidence': confidence,
                'source': source,
                'count': 1,
                'last_used': time.time()
            }
    
    def export_for_review(self) -> List[Dict]:
        """å¯¼å‡ºé«˜é¢‘å®ä½“ä¾›äººå·¥å®¡æ ¸"""
        high_freq_entities = []
        for entity, info in self.verified_entities.items():
            if info['count'] >= 3:  # å‡ºç°3æ¬¡ä»¥ä¸Š
                high_freq_entities.append({
                    'entity': entity,
                    'type': info['type'],
                    'confidence': info['confidence'],
                    'frequency': info['count'],
                    'source': info['source']
                })
        return sorted(high_freq_entities, key=lambda x: x['frequency'], reverse=True)

class EnhancedEntityTypeInferer:
    """å¢å¼ºçš„åˆ†å±‚å®ä½“ç±»å‹æ¨æ–­å™¨ï¼ˆæ”¯æŒå¤šé¢†åŸŸï¼‰"""

    def __init__(self, domain_name: str = "graph_algorithms"):
        # å®ä½“ç±»å‹ç¼“å­˜
        self.entity_cache = EntityTypeCache()

        # å½“å‰é¢†åŸŸ
        self.current_domain = domain_name

        # é¢†åŸŸæ¨¡æ¿ç®¡ç†å™¨
        from domain.domain_template import template_manager
        self.template_manager = template_manager

        # å¤šçº§ç´¢å¼•
        from indexing.multi_level_index import multi_level_index
        self.multi_index = multi_level_index

        # åˆå§‹åŒ–åŸºç¡€ç»“æ„
        self._init_base_structures()

        # åŠ¨æ€åŠ è½½é¢†åŸŸé…ç½®
        self._load_domain_config()

    def _init_base_structures(self):
        """åˆå§‹åŒ–åŸºç¡€æ•°æ®ç»“æ„"""
        # å…¼å®¹æ€§ï¼šä¿ç•™åŸæœ‰çš„å…³é”®è¯ç»“æ„
        self.type_keywords = {
            'Algorithm': [
                'algorithm', 'method', 'approach', 'technique', 'procedure',
                'search', 'sort', 'optimization', 'learning', 'training',
                'greedy', 'heuristic', 'recursive', 'iterative', 'dynamic'
            ],
            'Framework': [
                'framework', 'library', 'platform', 'toolkit', 'system',
                'network', 'model', 'architecture', 'neural', 'deep'
            ],
            'Technique': [
                'technique', 'strategy', 'mechanism', 'process', 'procedure',
                'pruning', 'compression', 'embedding', 'representation'
            ],
            'Task': [
                'task', 'problem', 'application', 'scenario', 'challenge',
                'prediction', 'classification', 'reasoning', 'analysis',
                # ä¸­æ–‡å…³é”®è¯
                'åˆ†æ', 'é¢„æµ‹', 'è¯†åˆ«', 'æ£€æµ‹', 'æ¨è', 'ç³»ç»Ÿ', 'ä»»åŠ¡'
            ],
            'Metric': [
                'metric', 'measure', 'score', 'rate', 'accuracy', 'precision',
                'recall', 'performance', 'quality', 'efficiency'
            ]
        }
        
        # å‘½åæ¨¡å¼ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
        self.type_patterns = {
            'Algorithm': [
                r'^Graph[A-Z][a-zA-Z]*$',      # GraphSAGE, GraphSAINT
                r'^[A-Z]{2,}$',                # GAT, GCN, GNN
                r'.*[Aa]lgorithm$',            # PageRankAlgorithm
                r'.*[Ss]earch$',               # BreadthFirstSearch
                r'.*[Nn]et$',                  # ResNet, DenseNet
                r'.*SAGE$',                    # GraphSAGE
            ],
            'Framework': [
                r'.*[Ff]ramework$',            # TensorFlow
                r'.*[Nn]etwork$',              # Neural Network
                r'.*[Ss]ystem$',               # RecommendationSystem
                r'.*[Ll]ibrary$',              # GraphLibrary
            ],
            'Task': [
                r'.*[Pp]rediction$',           # LinkPrediction
                r'.*[Cc]lassification$',       # NodeClassification
                r'.*[Aa]nalysis$',             # ç¤¾äº¤ç½‘ç»œåˆ†æ
                r'.*[Rr]ecognition$',          # PatternRecognition
                # ä¸­æ–‡æ¨¡å¼
                r'.*åˆ†æ$',                     # ç¤¾äº¤ç½‘ç»œåˆ†æ
                r'.*é¢„æµ‹$',                     # åˆ†å­æ€§è´¨é¢„æµ‹
                r'.*ç³»ç»Ÿ$',                     # æ¨èç³»ç»Ÿ
                r'.*å›¾è°±$',                     # çŸ¥è¯†å›¾è°±
            ],
            'Technique': [
                r'.*[Pp]runing$',              # NetworkPruning
                r'.*[Cc]ompression$',          # ModelCompression
                r'.*[Ee]mbedding$',            # GraphEmbedding
            ]
        }
        
        # ä¸Šä¸‹æ–‡æŒ‡ç¤ºè¯
        self.context_indicators = {
            'Algorithm': [
                'algorithm', 'method', 'approach', 'technique',
                'implements', 'proposes', 'introduces', 'develops',
                'solves', 'optimizes', 'searches', 'computes'
            ],
            'Framework': [
                'framework', 'library', 'platform', 'toolkit',
                'based on', 'built on', 'uses', 'leverages',
                'powered by', 'implemented in'
            ],
            'Task': [
                'task', 'problem', 'application', 'scenario',
                'used for', 'applied to', 'addresses', 'tackles',
                'solves', 'handles', 'performs'
            ],
            'Technique': [
                'technique', 'strategy', 'mechanism', 'process',
                'applies', 'employs', 'utilizes', 'adopts'
            ]
        }

    def _load_domain_config(self):
        """åŠ è½½é¢†åŸŸé…ç½®"""
        template = self.template_manager.get_template(self.current_domain)
        if template:
            # æ›´æ–°å…³é”®è¯
            if template.keywords:
                self.type_keywords.update(template.keywords)

            # æ›´æ–°æ¨¡å¼
            if template.patterns:
                self.type_patterns.update(template.patterns)

            # æ›´æ–°ä¸Šä¸‹æ–‡æŒ‡ç¤ºè¯
            if template.context_indicators:
                self.context_indicators.update(template.context_indicators)

            print(f"âœ… åŠ è½½é¢†åŸŸé…ç½®: {template.display_name}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°é¢†åŸŸæ¨¡æ¿: {self.current_domain}")

    def switch_domain(self, domain_name: str):
        """åˆ‡æ¢é¢†åŸŸ"""
        self.current_domain = domain_name
        self._load_domain_config()
        print(f"ğŸ”„ åˆ‡æ¢åˆ°é¢†åŸŸ: {domain_name}")

    def infer_entity_type(self, entity_name: str, context: str = "") -> InferenceResult:
        """åˆ†å±‚å®ä½“ç±»å‹æ¨æ–­"""
        start_time = time.time()
        
        # ç¬¬1å±‚ï¼šç¡¬åŒ¹é…ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
        cached_result = self._cache_lookup(entity_name)
        if cached_result:
            return InferenceResult(
                entity_type=cached_result['type'],
                confidence=cached_result['confidence'],
                method='cache',
                inference_time=time.time() - start_time
            )
        
        # ç¬¬2å±‚ï¼šå¤šçº§ç´¢å¼•æœç´¢ï¼ˆå¿«é€Ÿä¸”å‡†ç¡®ï¼‰
        index_results = self.multi_index.search(entity_name, top_k=1)
        if index_results:
            best_type, confidence = index_results[0]
            return InferenceResult(
                entity_type=best_type,
                confidence=confidence,
                method='index',
                inference_time=time.time() - start_time
            )

        # ç¬¬3å±‚ï¼šæ¨¡å¼åŒ¹é…ï¼ˆå¿«ï¼‰
        pattern_result = self._pattern_match(entity_name)
        if pattern_result:
            return InferenceResult(
                entity_type=pattern_result,
                confidence=0.8,
                method='pattern',
                inference_time=time.time() - start_time
            )

        # ç¬¬4å±‚ï¼šå…³é”®è¯åŒ¹é…ï¼ˆä¸­ç­‰ï¼‰
        keyword_result = self._keyword_match(entity_name)
        if keyword_result:
            return InferenceResult(
                entity_type=keyword_result,
                confidence=0.7,
                method='keyword',
                inference_time=time.time() - start_time
            )
        
        # ç¬¬5å±‚ï¼šä¸Šä¸‹æ–‡æ¨æ–­ï¼ˆæ…¢ï¼‰
        if context:
            context_result = self._context_infer(entity_name, context)
            if context_result:
                return InferenceResult(
                    entity_type=context_result,
                    confidence=0.6,
                    method='context',
                    inference_time=time.time() - start_time
                )

        # ç¬¬6å±‚ï¼šæœªçŸ¥ç±»å‹ï¼ˆäº¤ç»™LLMå¤„ç†ï¼‰
        return InferenceResult(
            entity_type=None,
            confidence=0.0,
            method='unknown',
            inference_time=time.time() - start_time
        )
    
    def _cache_lookup(self, entity_name: str) -> Optional[Dict]:
        """ç¼“å­˜æŸ¥æ‰¾"""
        cached = self.entity_cache.get(entity_name)
        if cached and cached['confidence'] > self.entity_cache.confidence_threshold:
            return cached
        return None
    
    def _pattern_match(self, entity_name: str) -> Optional[str]:
        """åŸºäºå‘½åæ¨¡å¼çš„ç±»å‹æ¨æ–­"""
        for entity_type, patterns in self.type_patterns.items():
            for pattern in patterns:
                if re.match(pattern, entity_name, re.IGNORECASE):
                    return entity_type
        return None
    
    def _keyword_match(self, entity_name: str) -> Optional[str]:
        """åŸºäºå…³é”®è¯çš„ç±»å‹æ¨æ–­"""
        entity_lower = entity_name.lower()
        
        # è®¡ç®—æ¯ç§ç±»å‹çš„åŒ¹é…åˆ†æ•°
        type_scores = defaultdict(int)
        
        for entity_type, keywords in self.type_keywords.items():
            for keyword in keywords:
                if keyword in entity_lower:
                    # æ ¹æ®åŒ¹é…é•¿åº¦ç»™åˆ†
                    score = len(keyword)
                    type_scores[entity_type] += score
        
        if type_scores:
            # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
            best_type = max(type_scores.items(), key=lambda x: x[1])
            return best_type[0]
        
        return None
    
    def _context_infer(self, entity_name: str, context: str) -> Optional[str]:
        """åŸºäºä¸Šä¸‹æ–‡çš„ç±»å‹æ¨æ–­"""
        context_lower = context.lower()
        
        # è®¡ç®—ä¸Šä¸‹æ–‡æŒ‡ç¤ºè¯åŒ¹é…åˆ†æ•°
        type_scores = defaultdict(int)
        
        for entity_type, indicators in self.context_indicators.items():
            for indicator in indicators:
                if indicator in context_lower:
                    type_scores[entity_type] += 1
        
        if type_scores:
            # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
            best_type = max(type_scores.items(), key=lambda x: x[1])
            return best_type[0]
        
        return None
    
    def update_cache_from_validation(self, entity_name: str, entity_type: str, 
                                   confidence: float, source: str):
        """ä»éªŒè¯ç»“æœæ›´æ–°ç¼“å­˜"""
        self.entity_cache.add_verified_entity(entity_name, entity_type, confidence, source)
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_entities = len(self.entity_cache.verified_entities)
        high_freq_count = len([e for e in self.entity_cache.verified_entities.values() if e['count'] >= 3])
        
        return {
            'total_cached_entities': total_entities,
            'high_frequency_entities': high_freq_count,
            'cache_hit_potential': high_freq_count / total_entities if total_entities > 0 else 0
        }

# å‘åå…¼å®¹çš„æ¥å£
class EntityTypeInferer(EnhancedEntityTypeInferer):
    """å‘åå…¼å®¹çš„å®ä½“ç±»å‹æ¨æ–­å™¨"""
    
    def _infer_type_from_name(self, entity_name: str) -> Optional[str]:
        """å‘åå…¼å®¹çš„æ–¹æ³•"""
        result = self.infer_entity_type(entity_name)
        return result.entity_type
