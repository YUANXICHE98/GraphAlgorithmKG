"""
å¢å¼ºçš„å®ä½“ç±»å‹æ¨æ–­å™¨
åŸºäºåˆ†å±‚åŒ¹é…ç­–ç•¥çš„å®ä½“ç±»å‹æ¨æ–­ç³»ç»Ÿ
"""

import re
import time
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from dataclasses import dataclass

@dataclass
class InferenceResult:
    """æ¨æ–­ç»“æœ"""
    entity_type: Optional[str]
    confidence: float
    method: str  # 'cache', 'pattern', 'keyword', 'context', 'unknown'
    inference_time: float

class EntityTypeCache:
    """å®ä½“ç±»å‹ç¼“å­˜"""
    
    def __init__(self):
        self.verified_entities = {}
        self.confidence_threshold = 0.9
    
    def get(self, entity_name: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„å®ä½“ç±»å‹"""
        return self.verified_entities.get(entity_name.lower())
    
    def add_verified_entity(self, entity_name: str, entity_type: str, 
                          confidence: float, source: str):
        """æ·»åŠ å·²éªŒè¯çš„å®ä½“ç±»å‹"""
        key = entity_name.lower()
        if key in self.verified_entities:
            # æ›´æ–°ä½¿ç”¨è®¡æ•°
            self.verified_entities[key]['count'] += 1
            self.verified_entities[key]['last_used'] = time.time()
        else:
            self.verified_entities[key] = {
                'type': entity_type,
                'confidence': confidence,
                'source': source,
                'count': 1,
                'last_used': time.time()
            }
    
    def export_for_review(self) -> List[Dict]:
        """å¯¼å‡ºé«˜é¢‘å®ä½“ä¾›äººå·¥å®¡æ ¸"""
        high_freq_entities = []
        for entity, info in self.verified_entities.items():
            if info['count'] >= 3:  # å‡ºç°3æ¬¡ä»¥ä¸Š
                high_freq_entities.append({
                    'entity': entity,
                    'type': info['type'],
                    'confidence': info['confidence'],
                    'frequency': info['count'],
                    'source': info['source']
                })
        return sorted(high_freq_entities, key=lambda x: x['frequency'], reverse=True)

class EnhancedEntityTypeInferer:
    """å¢å¼ºçš„åˆ†å±‚å®ä½“ç±»å‹æ¨æ–­å™¨ï¼ˆæ”¯æŒå¤šé¢†åŸŸï¼‰"""

    def __init__(self, domain_name: str = "graph_algorithms"):
        # å®ä½“ç±»å‹ç¼“å­˜
        self.entity_cache = EntityTypeCache()

        # å½“å‰é¢†åŸŸ
        self.current_domain = domain_name

        # ä½¿ç”¨æœ¬ä½“Schemaç³»ç»Ÿ
        from ontology.managers.dynamic_schema import DynamicOntologyManager
        self.ontology_manager = DynamicOntologyManager()

        # åˆå§‹åŒ–åŸºç¡€ç»“æ„
        self._init_base_structures()

        # åŠ¨æ€åŠ è½½é¢†åŸŸé…ç½®
        self._load_domain_config()

    def _init_base_structures(self):
        """åˆå§‹åŒ–åŸºç¡€æ•°æ®ç»“æ„"""
        # å…¼å®¹æ€§ï¼šä¿ç•™åŸæœ‰çš„å…³é”®è¯ç»“æ„
        self.type_keywords = {
            'Algorithm': [
                'algorithm', 'method', 'approach', 'technique', 'procedure',
                'search', 'sort', 'optimization', 'learning', 'training',
                'greedy', 'heuristic', 'recursive', 'iterative', 'dynamic'
            ],
            'Framework': [
                'framework', 'library', 'platform', 'toolkit', 'system',
                'network', 'model', 'architecture', 'neural', 'deep'
            ],
            'Technique': [
                'technique', 'strategy', 'mechanism', 'process', 'procedure',
                'pruning', 'compression', 'embedding', 'representation'
            ],
            'Task': [
                'task', 'problem', 'application', 'scenario', 'challenge',
                'prediction', 'classification', 'reasoning', 'analysis',
                # ä¸­æ–‡å…³é”®è¯
                'åˆ†æ', 'é¢„æµ‹', 'è¯†åˆ«', 'æ£€æµ‹', 'æ¨è', 'ç³»ç»Ÿ', 'ä»»åŠ¡'
            ],
            'Metric': [
                'metric', 'measure', 'score', 'rate', 'accuracy', 'precision',
                'recall', 'performance', 'quality', 'efficiency'
            ]
        }
        
        # å‘½åæ¨¡å¼ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
        self.type_patterns = {
            'Algorithm': [
                r'^Graph[A-Z][a-zA-Z]*$',      # GraphSAGE, GraphSAINT
                r'^[A-Z]{2,}$',                # GAT, GCN, GNN
                r'.*[Aa]lgorithm$',            # PageRankAlgorithm
                r'.*[Ss]earch$',               # BreadthFirstSearch
                r'.*[Nn]et$',                  # ResNet, DenseNet
                r'.*SAGE$',                    # GraphSAGE
            ],
            'Framework': [
                r'.*[Ff]ramework$',            # TensorFlow
                r'.*[Nn]etwork$',              # Neural Network
                r'.*[Ss]ystem$',               # RecommendationSystem
                r'.*[Ll]ibrary$',              # GraphLibrary
            ],
            'Task': [
                r'.*[Pp]rediction$',           # LinkPrediction
                r'.*[Cc]lassification$',       # NodeClassification
                r'.*[Aa]nalysis$',             # ç¤¾äº¤ç½‘ç»œåˆ†æ
                r'.*[Rr]ecognition$',          # PatternRecognition
                # ä¸­æ–‡æ¨¡å¼
                r'.*åˆ†æ$',                     # ç¤¾äº¤ç½‘ç»œåˆ†æ
                r'.*é¢„æµ‹$',                     # åˆ†å­æ€§è´¨é¢„æµ‹
                r'.*ç³»ç»Ÿ$',                     # æ¨èç³»ç»Ÿ
                r'.*å›¾è°±$',                     # çŸ¥è¯†å›¾è°±
            ],
            'Technique': [
                r'.*[Pp]runing$',              # NetworkPruning
                r'.*[Cc]ompression$',          # ModelCompression
                r'.*[Ee]mbedding$',            # GraphEmbedding
            ]
        }
        
        # ä¸Šä¸‹æ–‡æŒ‡ç¤ºè¯
        self.context_indicators = {
            'Algorithm': [
                'algorithm', 'method', 'approach', 'technique',
                'implements', 'proposes', 'introduces', 'develops',
                'solves', 'optimizes', 'searches', 'computes'
            ],
            'Framework': [
                'framework', 'library', 'platform', 'toolkit',
                'based on', 'built on', 'uses', 'leverages',
                'powered by', 'implemented in'
            ],
            'Task': [
                'task', 'problem', 'application', 'scenario',
                'used for', 'applied to', 'addresses', 'tackles',
                'solves', 'handles', 'performs'
            ],
            'Technique': [
                'technique', 'strategy', 'mechanism', 'process',
                'applies', 'employs', 'utilizes', 'adopts'
            ]
        }

    def _load_domain_config(self):
        """ä»æœ¬ä½“SchemaåŠ è½½é…ç½®"""
        # ä»æœ¬ä½“Schemaä¸­è·å–å®ä½“ç±»å‹å’Œå…³ç³»é…ç½®
        entity_types = self.ontology_manager.entity_types

        # æ›´æ–°å…³é”®è¯å’Œæ¨¡å¼
        for entity_type, config in entity_types.items():
            if hasattr(config, 'keywords') and config.keywords:
                self.type_keywords[entity_type] = config.keywords
            if hasattr(config, 'patterns') and config.patterns:
                self.type_patterns[entity_type] = config.patterns

        print(f"âœ… ä»æœ¬ä½“SchemaåŠ è½½é…ç½®: {len(entity_types)} ä¸ªå®ä½“ç±»å‹")

    def switch_domain(self, schema_file: str):
        """åˆ‡æ¢Schemaé…ç½®"""
        from ontology.managers.dynamic_schema import DynamicOntologyManager
        self.ontology_manager = DynamicOntologyManager(schema_file)
        self._load_domain_config()
        print(f"ğŸ”„ åˆ‡æ¢åˆ°Schema: {schema_file}")

    def infer_entity_type(self, entity_name: str, context: str = "") -> InferenceResult:
        """åˆ†å±‚å®ä½“ç±»å‹æ¨æ–­"""
        start_time = time.time()
        
        # ç¬¬1å±‚ï¼šç¡¬åŒ¹é…ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
        cached_result = self._cache_lookup(entity_name)
        if cached_result:
            return InferenceResult(
                entity_type=cached_result['type'],
                confidence=cached_result['confidence'],
                method='cache',
                inference_time=time.time() - start_time
            )
        
        # ç¬¬2å±‚ï¼šæœ¬ä½“SchemaåŒ¹é…ï¼ˆå¿«é€Ÿä¸”å‡†ç¡®ï¼‰
        schema_result = self._schema_match(entity_name)
        if schema_result:
            return InferenceResult(
                entity_type=schema_result,
                confidence=0.9,
                method='schema',
                inference_time=time.time() - start_time
            )

        # ç¬¬3å±‚ï¼šæ¨¡å¼åŒ¹é…ï¼ˆå¿«ï¼‰
        pattern_result = self._pattern_match(entity_name)
        if pattern_result:
            return InferenceResult(
                entity_type=pattern_result,
                confidence=0.8,
                method='pattern',
                inference_time=time.time() - start_time
            )

        # ç¬¬4å±‚ï¼šå…³é”®è¯åŒ¹é…ï¼ˆä¸­ç­‰ï¼‰
        keyword_result = self._keyword_match(entity_name)
        if keyword_result:
            return InferenceResult(
                entity_type=keyword_result,
                confidence=0.7,
                method='keyword',
                inference_time=time.time() - start_time
            )
        
        # ç¬¬5å±‚ï¼šä¸Šä¸‹æ–‡æ¨æ–­ï¼ˆæ…¢ï¼‰
        if context:
            context_result = self._context_infer(entity_name, context)
            if context_result:
                return InferenceResult(
                    entity_type=context_result,
                    confidence=0.6,
                    method='context',
                    inference_time=time.time() - start_time
                )

        # ç¬¬6å±‚ï¼šæœªçŸ¥ç±»å‹ï¼ˆäº¤ç»™LLMå¤„ç†ï¼‰
        return InferenceResult(
            entity_type=None,
            confidence=0.0,
            method='unknown',
            inference_time=time.time() - start_time
        )
    
    def _cache_lookup(self, entity_name: str) -> Optional[Dict]:
        """ç¼“å­˜æŸ¥æ‰¾"""
        cached = self.entity_cache.get(entity_name)
        if cached and cached['confidence'] > self.entity_cache.confidence_threshold:
            return cached
        return None

    def _schema_match(self, entity_name: str) -> Optional[str]:
        """åŸºäºæœ¬ä½“Schemaçš„åŒ¹é…"""
        entity_lower = entity_name.lower()

        # æ£€æŸ¥Schemaä¸­å®šä¹‰çš„å®ä½“ç±»å‹
        for entity_type, config in self.ontology_manager.entity_types.items():
            # æ£€æŸ¥å…³é”®è¯
            if hasattr(config, 'keywords'):
                for keyword in config.keywords:
                    if keyword.lower() in entity_lower:
                        return entity_type

            # æ£€æŸ¥æ¨¡å¼
            if hasattr(config, 'patterns'):
                for pattern in config.patterns:
                    try:
                        if re.match(pattern, entity_name, re.IGNORECASE):
                            return entity_type
                    except re.error:
                        continue

        return None
    
    def _pattern_match(self, entity_name: str) -> Optional[str]:
        """åŸºäºå‘½åæ¨¡å¼çš„ç±»å‹æ¨æ–­"""
        for entity_type, patterns in self.type_patterns.items():
            for pattern in patterns:
                if re.match(pattern, entity_name, re.IGNORECASE):
                    return entity_type
        return None
    
    def _keyword_match(self, entity_name: str) -> Optional[str]:
        """åŸºäºå…³é”®è¯çš„ç±»å‹æ¨æ–­"""
        entity_lower = entity_name.lower()
        
        # è®¡ç®—æ¯ç§ç±»å‹çš„åŒ¹é…åˆ†æ•°
        type_scores = defaultdict(int)
        
        for entity_type, keywords in self.type_keywords.items():
            for keyword in keywords:
                if keyword in entity_lower:
                    # æ ¹æ®åŒ¹é…é•¿åº¦ç»™åˆ†
                    score = len(keyword)
                    type_scores[entity_type] += score
        
        if type_scores:
            # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
            best_type = max(type_scores.items(), key=lambda x: x[1])
            return best_type[0]
        
        return None
    
    def _context_infer(self, entity_name: str, context: str) -> Optional[str]:
        """åŸºäºä¸Šä¸‹æ–‡çš„ç±»å‹æ¨æ–­"""
        context_lower = context.lower()
        
        # è®¡ç®—ä¸Šä¸‹æ–‡æŒ‡ç¤ºè¯åŒ¹é…åˆ†æ•°
        type_scores = defaultdict(int)
        
        for entity_type, indicators in self.context_indicators.items():
            for indicator in indicators:
                if indicator in context_lower:
                    type_scores[entity_type] += 1
        
        if type_scores:
            # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
            best_type = max(type_scores.items(), key=lambda x: x[1])
            return best_type[0]
        
        return None
    
    def update_cache_from_validation(self, entity_name: str, entity_type: str, 
                                   confidence: float, source: str):
        """ä»éªŒè¯ç»“æœæ›´æ–°ç¼“å­˜"""
        self.entity_cache.add_verified_entity(entity_name, entity_type, confidence, source)
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_entities = len(self.entity_cache.verified_entities)
        high_freq_count = len([e for e in self.entity_cache.verified_entities.values() if e['count'] >= 3])
        
        return {
            'total_cached_entities': total_entities,
            'high_frequency_entities': high_freq_count,
            'cache_hit_potential': high_freq_count / total_entities if total_entities > 0 else 0
        }

# å‘åå…¼å®¹çš„æ¥å£
class EntityTypeInferer(EnhancedEntityTypeInferer):
    """å‘åå…¼å®¹çš„å®ä½“ç±»å‹æ¨æ–­å™¨"""
    
    def _infer_type_from_name(self, entity_name: str) -> Optional[str]:
        """å‘åå…¼å®¹çš„æ–¹æ³•"""
        result = self.infer_entity_type(entity_name)
        return result.entity_type
