"""
LLMå®¢æˆ·ç«¯æ¨¡å—
æä¾›ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£ï¼Œæ”¯æŒå¤šç§LLMæœåŠ¡
"""

import os
import json
import time
from typing import Optional, Dict, Any
from dataclasses import dataclass
from pathlib import Path

@dataclass
class LLMResponse:
    """LLMå“åº”ç»“æœ"""
    content: str
    usage: Dict[str, int]
    model: str
    response_time: float

class LLMClient:
    """LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§LLMæœåŠ¡"""
    
    def __init__(self, model: str = "gpt-3.5-turbo", api_key: Optional[str] = None):
        # é¦–å…ˆå°è¯•ä»config.jsonè¯»å–é…ç½®
        config = self._load_config()

        self.model = model
        self.api_key = api_key or config.get("openai", {}).get("api_key") or os.getenv("OPENAI_API_KEY")
        self.base_url = config.get("openai", {}).get("base_url") or os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        
        # å¦‚æœæ²¡æœ‰APIå¯†é’¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
        self.mock_mode = not self.api_key
        
        if self.mock_mode:
            print("âš ï¸ LLMå®¢æˆ·ç«¯è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæœªé…ç½®APIå¯†é’¥ï¼‰")
        else:
            print(f"âœ… LLMå®¢æˆ·ç«¯å·²é…ç½®: {self.model} @ {self.base_url}")

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = Path("config.json")
        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return {}
    
    def generate_response(self, prompt: str, max_tokens: int = 1000, 
                         temperature: float = 0.7) -> str:
        """
        ç”ŸæˆLLMå“åº”
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            LLMç”Ÿæˆçš„æ–‡æœ¬
        """
        start_time = time.time()
        
        if self.mock_mode:
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼šåŸºäºç®€å•è§„åˆ™ç”Ÿæˆå“åº”
            return self._mock_response(prompt)
        
        try:
            # å°è¯•ä½¿ç”¨OpenAI API
            response = self._call_openai_api(prompt, max_tokens, temperature)
            return response
        except Exception as e:
            print(f"âš ï¸ LLM APIè°ƒç”¨å¤±è´¥: {e}")
            # é™çº§åˆ°æ¨¡æ‹Ÿæ¨¡å¼
            return self._mock_response(prompt)
    
    def _mock_response(self, prompt: str) -> str:
        """æ¨¡æ‹ŸLLMå“åº”"""
        prompt_lower = prompt.lower()

        # Schemaé€‰æ‹©çš„æ¨¡æ‹Ÿé€»è¾‘
        if "schema" in prompt_lower and "choose" in prompt_lower:
            if "temporal" in prompt_lower or "spatial" in prompt_lower or "time" in prompt_lower:
                return "spatiotemporal_schema.yaml"
            elif "architecture" in prompt_lower or "framework" in prompt_lower:
                return "../schema_config.yaml"
            else:
                return "spatiotemporal_schema.yaml"
        
        # ä¸‰å…ƒç»„æŠ½å–çš„æ¨¡æ‹Ÿé€»è¾‘ - æ›´å®½æ³›çš„åŒ¹é…æ¡ä»¶
        if ("æŠ½å–" in prompt_lower and "ä¸‰å…ƒç»„" in prompt_lower) or ("extract" in prompt_lower and "triple" in prompt_lower) or "jsonæ ¼å¼è¿”å›ä¸‰å…ƒç»„" in prompt_lower:
            # æ ¹æ®æ–‡æœ¬å†…å®¹ç”Ÿæˆæ›´æ™ºèƒ½çš„æ¨¡æ‹Ÿä¸‰å…ƒç»„
            if "dodaf" in prompt_lower or "architecture" in prompt_lower:
                return """[
                    {"subject": "DoDAF", "predicate": "is_instance_of", "object": "Framework", "confidence": 0.9},
                    {"subject": "Operational View", "predicate": "is_instance_of", "object": "Framework", "confidence": 0.85},
                    {"subject": "OV-1", "predicate": "is_instance_of", "object": "Algorithm", "confidence": 0.8}
                ]"""
            elif "temperature" in prompt_lower or "monitor" in prompt_lower:
                return """[
                    {"subject": "checkTemperature", "predicate": "is_instance_of", "object": "Action", "confidence": 0.9},
                    {"subject": "temperature > 30", "predicate": "is_instance_of", "object": "Condition", "confidence": 0.85},
                    {"subject": "turnOnAirConditioner", "predicate": "is_instance_of", "object": "Outcome", "confidence": 0.8},
                    {"subject": "checkTemperature", "predicate": "hasCondition", "object": "temperature > 30", "confidence": 0.8},
                    {"subject": "checkTemperature", "predicate": "resultsIn", "object": "turnOnAirConditioner", "confidence": 0.8}
                ]"""
            elif "æ—¶ç©º" in prompt_lower or "æ´ªæ°´" in prompt_lower or "æ°´åˆ©" in prompt_lower:
                return """[
                    {"subject": "æ´ªæ°´äº‹ä»¶", "predicate": "is_instance_of", "object": "Event", "confidence": 0.9},
                    {"subject": "é•¿æ±ŸæµåŸŸ", "predicate": "is_instance_of", "object": "SpatialEntity", "confidence": 0.85},
                    {"subject": "2024-10-23 08:00", "predicate": "is_instance_of", "object": "TemporalEntity", "confidence": 0.8},
                    {"subject": "æ´ªæ°´äº‹ä»¶", "predicate": "locatedAt", "object": "é•¿æ±ŸæµåŸŸ", "confidence": 0.8},
                    {"subject": "æ´ªæ°´äº‹ä»¶", "predicate": "hasStartTime", "object": "2024-10-23 08:00", "confidence": 0.8}
                ]"""
            else:
                return """[
                    {"subject": "ç¤ºä¾‹å®ä½“", "predicate": "is_instance_of", "object": "ç¤ºä¾‹ç±»å‹", "confidence": 0.7},
                    {"subject": "å®ä½“A", "predicate": "relates_to", "object": "å®ä½“B", "confidence": 0.7}
                ]"""
        
        # è¯­ä¹‰éªŒè¯çš„æ¨¡æ‹Ÿé€»è¾‘
        if "validate" in prompt_lower or "verify" in prompt_lower:
            return "valid"
        
        # é»˜è®¤å“åº”
        return "æ¨¡æ‹ŸLLMå“åº”ï¼šåŸºäºè¾“å…¥å†…å®¹çš„æ™ºèƒ½åˆ†æç»“æœ"
    
    def _call_openai_api(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """è°ƒç”¨OpenAI API"""
        try:
            import openai
            
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            return response.choices[0].message.content
            
        except ImportError:
            raise Exception("OpenAIåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")
        except Exception as e:
            raise Exception(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
    
    def extract_triples(self, text: str, schema_info: Dict[str, Any]) -> list:
        """
        ä»æ–‡æœ¬ä¸­æŠ½å–ä¸‰å…ƒç»„

        Args:
            text: è¾“å…¥æ–‡æœ¬
            schema_info: Schemaä¿¡æ¯

        Returns:
            ä¸‰å…ƒç»„åˆ—è¡¨
        """
        schema_name = schema_info.get('name', 'æœªçŸ¥Schema')
        entity_types = list(schema_info.get('entity_types', {}).keys())
        relation_types = list(schema_info.get('relation_types', {}).keys())

        # åœ¨æ¨¡æ‹Ÿæ¨¡å¼ä¸‹ï¼Œæ ¹æ®Schemaç±»å‹å’Œæ–‡æœ¬å†…å®¹ç”Ÿæˆä¸åŒçš„ä¸‰å…ƒç»„
        if self.mock_mode:
            return self._generate_mock_triples(text, schema_name, entity_types, relation_types)

        prompt = f"""
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–ç¬¦åˆ{schema_name}çš„çŸ¥è¯†ä¸‰å…ƒç»„ã€‚

å¯ç”¨å®ä½“ç±»å‹: {', '.join(entity_types[:10])}
å¯ç”¨å…³ç³»ç±»å‹: {', '.join(relation_types[:10])}

æ–‡æœ¬å†…å®¹:
{text}

è¯·ä»¥JSONæ ¼å¼è¿”å›ä¸‰å…ƒç»„åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹:
[
    {{"subject": "ä¸»è¯­", "predicate": "å…³ç³»", "object": "å®¾è¯­", "confidence": 0.9}},
    ...
]

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""

        response = self.generate_response(prompt, max_tokens=2000, temperature=0.3)

        print(f"ğŸ¤– LLMåŸå§‹å“åº”: {response[:200]}...")

        try:
            # å°è¯•è§£æJSON
            if response.strip().startswith('['):
                triples = json.loads(response)
                print(f"âœ… æˆåŠŸè§£æJSONï¼Œè·å¾— {len(triples)} ä¸ªä¸‰å…ƒç»„")
                return triples
            else:
                print(f"âš ï¸ LLMè¿”å›çš„ä¸æ˜¯JSONæ ¼å¼: {response[:100]}...")
                return []
        except json.JSONDecodeError as e:
            print(f"âš ï¸ LLMè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆJSON: {e}")
            print(f"   åŸå§‹å“åº”: {response[:200]}...")
            return []

    def _generate_mock_triples(self, text: str, schema_name: str, entity_types: list, relation_types: list) -> list:
        """æ ¹æ®Schemaç±»å‹ç”Ÿæˆæ¨¡æ‹Ÿä¸‰å…ƒç»„"""
        text_lower = text.lower()

        print(f"ğŸ­ æ¨¡æ‹Ÿæ¨¡å¼: æ ¹æ®Schema '{schema_name}' ç”Ÿæˆä¸‰å…ƒç»„")

        # é€šç”¨Schemaçš„æ¨¡æ‹Ÿä¸‰å…ƒç»„
        if "é€šç”¨" in schema_name or "general" in schema_name.lower():
            if "dodaf" in text_lower and "architecture" in text_lower:
                return [
                    {"subject": "DoDAF", "predicate": "is_instance_of", "object": "Framework", "confidence": 0.9},
                    {"subject": "Operational View", "predicate": "is_instance_of", "object": "Framework", "confidence": 0.85},
                    {"subject": "OV-1", "predicate": "is_instance_of", "object": "Algorithm", "confidence": 0.8},
                    {"subject": "System View", "predicate": "is_instance_of", "object": "Framework", "confidence": 0.85},
                    {"subject": "SV-1", "predicate": "is_instance_of", "object": "Algorithm", "confidence": 0.8}
                ]

        # æ—¶ç©ºSchemaçš„æ¨¡æ‹Ÿä¸‰å…ƒç»„
        elif "æ—¶ç©º" in schema_name or "spatiotemporal" in schema_name.lower():
            if "æ´ªæ°´" in text_lower or "æ°´åˆ©" in text_lower or "æ—¶ç©º" in text_lower:
                return [
                    {"subject": "æ´ªæ°´äº‹ä»¶", "predicate": "is_instance_of", "object": "Event", "confidence": 0.9},
                    {"subject": "é•¿æ±ŸæµåŸŸ", "predicate": "is_instance_of", "object": "SpatialEntity", "confidence": 0.85},
                    {"subject": "2024-10-23 08:00", "predicate": "is_instance_of", "object": "TemporalEntity", "confidence": 0.8},
                    {"subject": "4å°æ—¶", "predicate": "is_instance_of", "object": "TemporalEntity", "confidence": 0.8},
                    {"subject": "æ‰§è¡Œæ°´ä½ç›‘æµ‹", "predicate": "is_instance_of", "object": "Action", "confidence": 0.85},
                    {"subject": "æ´ªæ°´äº‹ä»¶", "predicate": "hasStartTime", "object": "2024-10-23 08:00", "confidence": 0.8},
                    {"subject": "æ´ªæ°´äº‹ä»¶", "predicate": "locatedAt", "object": "é•¿æ±ŸæµåŸŸ", "confidence": 0.8},
                    {"subject": "æ´ªæ°´äº‹ä»¶", "predicate": "hasDuration", "object": "4å°æ—¶", "confidence": 0.8}
                ]
            elif "checktemperature" in text_lower or "monitor" in text_lower or "do-da-f" in text_lower:
                return [
                    {"subject": "checkTemperature", "predicate": "is_instance_of", "object": "Action", "confidence": 0.9},
                    {"subject": "monitorWaterLevel", "predicate": "is_instance_of", "object": "Action", "confidence": 0.9},
                    {"subject": "executeFloodWarning", "predicate": "is_instance_of", "object": "Action", "confidence": 0.9},
                    {"subject": "temperature > 30", "predicate": "is_instance_of", "object": "Condition", "confidence": 0.85},
                    {"subject": "sensor_status == normal", "predicate": "is_instance_of", "object": "Condition", "confidence": 0.85},
                    {"subject": "turnOnAirConditioner", "predicate": "is_instance_of", "object": "Outcome", "confidence": 0.8},
                    {"subject": "generateWaterLevelReport", "predicate": "is_instance_of", "object": "Outcome", "confidence": 0.8},
                    {"subject": "checkTemperature", "predicate": "hasCondition", "object": "temperature > 30", "confidence": 0.8},
                    {"subject": "checkTemperature", "predicate": "resultsIn", "object": "turnOnAirConditioner", "confidence": 0.8}
                ]

        # é»˜è®¤è¿”å›åŸºç¡€ä¸‰å…ƒç»„
        return [
            {"subject": "ç¤ºä¾‹å®ä½“", "predicate": "is_instance_of", "object": entity_types[0] if entity_types else "Entity", "confidence": 0.7}
        ]
    
    def validate_triple(self, subject: str, predicate: str, obj: str, 
                       schema_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        éªŒè¯ä¸‰å…ƒç»„çš„è¯­ä¹‰æ­£ç¡®æ€§
        
        Args:
            subject: ä¸»è¯­
            predicate: è°“è¯­
            obj: å®¾è¯­
            schema_info: Schemaä¿¡æ¯
            
        Returns:
            éªŒè¯ç»“æœ
        """
        prompt = f"""
è¯·éªŒè¯ä»¥ä¸‹ä¸‰å…ƒç»„çš„è¯­ä¹‰æ­£ç¡®æ€§ï¼š

ä¸‰å…ƒç»„: ({subject}, {predicate}, {obj})
Schema: {schema_info.get('name', 'æœªçŸ¥')}

è¯·åˆ¤æ–­è¿™ä¸ªä¸‰å…ƒç»„æ˜¯å¦è¯­ä¹‰åˆç†ï¼Œå¹¶ç»™å‡ºç½®ä¿¡åº¦åˆ†æ•°(0-1)ã€‚

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœ:
{{"valid": true/false, "confidence": 0.8, "reason": "éªŒè¯ç†ç”±"}}
"""
        
        response = self.generate_response(prompt, max_tokens=200, temperature=0.1)
        
        try:
            if response.strip().startswith('{'):
                return json.loads(response)
            else:
                # é»˜è®¤è¿”å›æœ‰æ•ˆ
                return {"valid": True, "confidence": 0.5, "reason": "é»˜è®¤éªŒè¯"}
        except json.JSONDecodeError:
            return {"valid": True, "confidence": 0.5, "reason": "è§£æå¤±è´¥ï¼Œé»˜è®¤æœ‰æ•ˆ"}

# å…¨å±€LLMå®¢æˆ·ç«¯å®ä¾‹
llm_client = LLMClient()
