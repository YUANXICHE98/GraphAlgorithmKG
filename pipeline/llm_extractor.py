"""
LLMçŸ¥è¯†å›¾è°±æŠ½å–å™¨ v2.0
é›†æˆåŠ¨æ€Schemaå’ŒPromptæ¨¡æ¿
"""

import json
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from ontology.managers.dynamic_schema import DynamicOntologyManager
from pipeline.llm_validator import LLMSemanticValidator
from pipeline.stage_saver import stage_saver

@dataclass
class ExtractionResult:
    """æŠ½å–ç»“æœ"""
    success: bool
    triples: List[Dict] = None
    new_entities: List[str] = None
    new_relations: List[str] = None
    processing_time: float = 0.0
    error_message: str = ""

class LLMKGExtractor:
    """LLMçŸ¥è¯†å›¾è°±æŠ½å–å™¨ v2.0"""
    
    def __init__(self, ontology_manager: DynamicOntologyManager, model: str = "gpt-3.5-turbo"):
        self.ontology = ontology_manager
        self.model = model
        self.max_retries = 3
        self.timeout = 30
        self.validator = LLMSemanticValidator(ontology_manager, model)
    
    def extract_from_text(self, text: str, chunk_id: str = "", save_stages: bool = True) -> ExtractionResult:
        """ä»æ–‡æœ¬ä¸­æŠ½å–çŸ¥è¯†ä¸‰å…ƒç»„"""
        start_time = time.time()

        try:
            # è·å–åŠ¨æ€ç”Ÿæˆçš„Prompt
            prompts = self.ontology.get_llm_extraction_prompt(text)

            # è°ƒç”¨LLMï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„LLMæ¥å£å®ç°ï¼‰
            response = self._call_llm(prompts["system"], prompts["user"])

            # è§£æå“åº”
            result = self._parse_llm_response(response)

            # ä¿å­˜åŸå§‹æŠ½å–ç»“æœ
            if save_stages and result.get("triples"):
                raw_metadata = {
                    "text_length": len(text),
                    "chunk_id": chunk_id,
                    "model": self.model,
                    "llm_response_time": time.time() - start_time
                }
                stage_saver.save_stage("raw_extraction", result.get("triples", []), raw_metadata)

                # åˆ›å»ºæ‰‹å·¥å®¡æ ¸æ–‡ä»¶
                review_file = stage_saver.create_manual_review_file(
                    "LLMåŸå§‹æŠ½å–",
                    result.get("triples", [])
                )
                print(f"ğŸ“‹ æ‰‹å·¥å®¡æ ¸æ–‡ä»¶: {review_file}")
            
            # éªŒè¯æŠ½å–çš„ä¸‰å…ƒç»„ï¼ˆè§„åˆ™+è¯­ä¹‰ï¼‰
            validated_triples = []
            for triple in result.get("triples", []):
                validation = self.validator.validate_triple(
                    triple["subject"],
                    triple["predicate"],
                    triple["object"],
                    context=text[:200],  # æä¾›ä¸Šä¸‹æ–‡
                    use_llm=True
                )

                if validation.valid:
                    # æ·»åŠ éªŒè¯ä¿¡æ¯åˆ°ä¸‰å…ƒç»„
                    triple["validation_confidence"] = validation.confidence
                    triple["validation_method"] = "rule+semantic" if not validation.rule_validation["valid"] else "rule"
                    validated_triples.append(triple)
                else:
                    print(f"âš ï¸  ä¸‰å…ƒç»„æœ€ç»ˆéªŒè¯å¤±è´¥: {triple}")
                    print(f"   é—®é¢˜: {', '.join(validation.issues)}")

            # ä¿å­˜éªŒè¯åçš„ä¸‰å…ƒç»„
            if save_stages and validated_triples:
                validation_metadata = {
                    "original_count": len(result.get("triples", [])),
                    "validated_count": len(validated_triples),
                    "validation_rate": len(validated_triples) / len(result.get("triples", [])) if result.get("triples") else 0,
                    "chunk_id": chunk_id
                }
                stage_saver.save_stage("validated_triples", validated_triples, validation_metadata)

            processing_time = time.time() - start_time
            
            return ExtractionResult(
                success=True,
                triples=validated_triples,
                new_entities=result.get("new_entities", []),
                new_relations=result.get("new_relations", []),
                processing_time=processing_time
            )
            
        except Exception as e:
            return ExtractionResult(
                success=False,
                error_message=str(e),
                processing_time=time.time() - start_time
            )
    
    def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """è°ƒç”¨LLMæ¥å£"""
        try:
            # ä»é…ç½®æ–‡ä»¶è¯»å–APIè®¾ç½®
            import json
            with open('config.json', 'r') as f:
                config = json.load(f)
            
            openai_config = config.get('openai', {})
            
            if not openai_config.get('api_key'):
                print("âš ï¸  æœªé…ç½®OpenAI APIå¯†é’¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”")
                return self._mock_llm_response()
            
            from openai import OpenAI

            # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
            client = OpenAI(
                api_key=openai_config['api_key'],
                base_url=openai_config.get('base_url')
            )

            response = client.chat.completions.create(
                model=openai_config.get('model', self.model),
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=4000,
                timeout=self.timeout
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âš ï¸  LLMè°ƒç”¨å¤±è´¥: {e}")
            print("ä½¿ç”¨æ¨¡æ‹Ÿå“åº”ç»§ç»­æµ‹è¯•...")
            return self._mock_llm_response()
    
    def _mock_llm_response(self) -> str:
        """æ¨¡æ‹ŸLLMå“åº”ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        return json.dumps({
            "triples": [
                {
                    "subject": "MINERVA",
                    "predicate": "is_instance_of",
                    "object": "Algorithm",
                    "confidence": 0.9,
                    "evidence": "MINERVA is a reinforcement learning algorithm"
                }
            ],
            "new_entities": [],
            "new_relations": []
        }, ensure_ascii=False)
    
    def _parse_llm_response(self, response: str) -> Dict:
        """è§£æLLMå“åº”"""
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            # å°è¯•æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                raise ValueError(f"æ— æ³•è§£æLLMå“åº”: {response}")
