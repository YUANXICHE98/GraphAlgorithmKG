"""
åŸºäºSchemaçš„çŸ¥è¯†å›¾è°±æ„å»ºå™¨
æ”¯æŒJSONæ ¼å¼çš„ç»“æ„åŒ–çŸ¥è¯†å›¾è°±æ„å»ºï¼ŒåŒ…å«å®Œæ•´çš„å®ä½“å±æ€§å’Œå…³ç³»ä¿¡æ¯
"""

import json
import time
import uuid
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path

from ontology.managers.schema_detector import SchemaDetector
from ontology.managers.dynamic_schema import DynamicOntologyManager
from pipeline.enhanced_entity_inferer import EnhancedEntityTypeInferer
from pipeline.hybrid_triple_extractor import HybridTripleExtractor
from pipeline.session_manager import session_manager

@dataclass
class KGEntity:
    """çŸ¥è¯†å›¾è°±å®ä½“"""
    id: str
    name: str
    type: str
    properties: Dict[str, Any]
    confidence: float
    source: str
    created_at: str
    schema: str

@dataclass
class KGRelation:
    """çŸ¥è¯†å›¾è°±å…³ç³»"""
    id: str
    subject_id: str
    subject_name: str
    predicate: str
    object_id: str
    object_name: str
    properties: Dict[str, Any]
    confidence: float
    source: str
    created_at: str
    schema: str

@dataclass
class KnowledgeGraph:
    """å®Œæ•´çš„çŸ¥è¯†å›¾è°±"""
    metadata: Dict[str, Any]
    entities: List[KGEntity]
    relations: List[KGRelation]
    statistics: Dict[str, Any]

class SchemaBasedKGBuilder:
    """åŸºäºSchemaçš„çŸ¥è¯†å›¾è°±æ„å»ºå™¨"""
    
    def __init__(self):
        self.schema_detector = SchemaDetector()
        self.entity_inferer = EnhancedEntityTypeInferer()
        self.hybrid_extractor = None  # å»¶è¿Ÿåˆå§‹åŒ–

        # å®ä½“IDæ˜ å°„
        self.entity_id_map = {}
        
    def build_knowledge_graph(self, document_path: str,
                            output_path: Optional[str] = None) -> KnowledgeGraph:
        """
        ä»æ–‡æ¡£æ„å»ºå®Œæ•´çš„çŸ¥è¯†å›¾è°±

        Args:
            document_path: æ–‡æ¡£è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            å®Œæ•´çš„çŸ¥è¯†å›¾è°±å¯¹è±¡
        """
        start_time = time.time()

        print(f"\nğŸ—ï¸ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±")
        print(f"ğŸ“„ è¾“å…¥æ–‡æ¡£: {document_path}")
        print("=" * 80)

        # å¼€å§‹ä¼šè¯
        session_id = session_manager.start_session(document_path)

        # 1. è¯»å–æ–‡æ¡£
        print(f"\nğŸ“– æ­¥éª¤1: è¯»å–æ–‡æ¡£å†…å®¹")
        document_content = self._load_document(document_path)
        print(f"   ğŸ“ æ–‡æ¡£é•¿åº¦: {len(document_content)} å­—ç¬¦")
        print(f"   ğŸ“‹ æ–‡æ¡£é¢„è§ˆ: {document_content[:200]}...")

        # ä¿å­˜æ–‡æ¡£è¾“å…¥é˜¶æ®µ
        session_manager.save_stage_result(
            "document_input",
            {
                "document_path": document_path,
                "content": document_content,
                "content_length": len(document_content)
            },
            "åŸå§‹æ–‡æ¡£è¾“å…¥å’Œå†…å®¹è§£æ"
        )

        # 2. æ£€æµ‹Schema
        print(f"\nğŸ” æ­¥éª¤2: Schemaæ£€æµ‹")
        schema_results = self.schema_detector.detect_schema(document_content, use_llm=True)

        print(f"   ğŸ¯ å€™é€‰Schemaåˆ—è¡¨:")
        for i, result in enumerate(schema_results, 1):
            print(f"      {i}. {result.schema_file} (ç½®ä¿¡åº¦: {result.confidence:.3f}, æ–¹æ³•: {result.method})")
            print(f"         è¯æ®: {', '.join(result.evidence[:3])}")

        best_schema = self.schema_detector.get_best_schema(document_content)
        if not best_schema:
            raise ValueError("æ— æ³•æ£€æµ‹åˆ°åˆé€‚çš„Schema")

        print(f"   âœ… é€‰æ‹©Schema: {best_schema}")

        # ä¿å­˜Schemaæ£€æµ‹é˜¶æ®µ
        session_manager.save_stage_result(
            "schema_detection",
            {
                "candidates": [
                    {
                        "schema_file": result.schema_file,
                        "confidence": result.confidence,
                        "method": result.method,
                        "evidence": result.evidence
                    } for result in schema_results
                ],
                "selected_schema": best_schema
            },
            f"Schemaæ£€æµ‹å’Œé€‰æ‹©ï¼Œæœ€ç»ˆé€‰æ‹©: {best_schema}"
        )

        # 3. åˆ‡æ¢åˆ°å¯¹åº”çš„Schema
        print(f"\nâš™ï¸ æ­¥éª¤3: åˆ‡æ¢Schemaé…ç½®")
        self._switch_to_schema(best_schema)

        # æ˜¾ç¤ºSchemaä¿¡æ¯
        schema_info = {
            'name': self.entity_inferer.ontology_manager.metadata.get('name', 'æœªçŸ¥Schema'),
            'entity_types': list(self.entity_inferer.ontology_manager.entity_types.keys()),
            'relation_types': list(self.entity_inferer.ontology_manager.relation_types.keys())
        }
        print(f"   ğŸ“‹ Schemaåç§°: {schema_info['name']}")
        print(f"   ğŸ·ï¸ å®ä½“ç±»å‹ ({len(schema_info['entity_types'])}): {', '.join(schema_info['entity_types'][:5])}{'...' if len(schema_info['entity_types']) > 5 else ''}")
        print(f"   ğŸ”— å…³ç³»ç±»å‹ ({len(schema_info['relation_types'])}): {', '.join(schema_info['relation_types'][:5])}{'...' if len(schema_info['relation_types']) > 5 else ''}")

        # 4. æ··åˆä¸‰å…ƒç»„æŠ½å–ï¼ˆè§„åˆ™+LLMï¼‰
        print(f"\nğŸ”„ æ­¥éª¤4: æ··åˆä¸‰å…ƒç»„æŠ½å–")
        extraction_result = self._extract_triples_hybrid(document_content)
        triples = extraction_result.triples

        print(f"   ğŸ“Š æŠ½å–ç»Ÿè®¡:")
        print(f"      æ€»ä¸‰å…ƒç»„: {len(triples)} ä¸ª")
        print(f"      è§„åˆ™æŠ½å–: {extraction_result.rule_count} ä¸ª")
        print(f"      LLMæŠ½å–: {extraction_result.llm_count} ä¸ª")
        print(f"      ä½¿ç”¨æ–¹æ³•: {', '.join(extraction_result.methods_used)}")

        if triples:
            print(f"   ğŸ“‹ ä¸‰å…ƒç»„ç¤ºä¾‹:")
            for i, triple in enumerate(triples[:3], 1):
                method = triple.get('method', 'unknown')
                print(f"      {i}. ({triple['subject']}, {triple['predicate']}, {triple['object']}) - ç½®ä¿¡åº¦: {triple.get('confidence', 0.8):.2f} [{method}]")

        # ä¿å­˜ä¸‰å…ƒç»„æŠ½å–é˜¶æ®µ
        session_manager.save_stage_result(
            "triple_extraction",
            {
                "extraction_method": "hybrid",
                "rule_extraction_count": extraction_result.rule_count,
                "llm_extraction_count": extraction_result.llm_count,
                "methods_used": extraction_result.methods_used,
                "extraction_time": extraction_result.total_time,
                "triples": triples
            },
            f"æ··åˆä¸‰å…ƒç»„æŠ½å–ï¼Œè§„åˆ™:{extraction_result.rule_count}ä¸ªï¼ŒLLM:{extraction_result.llm_count}ä¸ª"
        )

        # 5. æ„å»ºå®ä½“å’Œå…³ç³»
        print(f"\nğŸ—ï¸ æ­¥éª¤5: æ„å»ºå®ä½“å’Œå…³ç³»å¯¹è±¡")
        entities, relations = self._build_entities_and_relations(triples, best_schema, document_path, extraction_result)
        print(f"   ğŸ·ï¸ ç”Ÿæˆå®ä½“: {len(entities)} ä¸ª")
        print(f"   ğŸ”— ç”Ÿæˆå…³ç³»: {len(relations)} ä¸ª")

        # æ˜¾ç¤ºå®ä½“ç±»å‹åˆ†å¸ƒ
        entity_type_counts = {}
        for entity in entities:
            entity_type_counts[entity.type] = entity_type_counts.get(entity.type, 0) + 1

        print(f"   ğŸ“Š å®ä½“ç±»å‹åˆ†å¸ƒ:")
        for entity_type, count in sorted(entity_type_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"      {entity_type}: {count} ä¸ª")

        # ä¿å­˜å®ä½“æ¨æ–­é˜¶æ®µ
        session_manager.save_stage_result(
            "entity_inference",
            {
                "entities": [asdict(entity) for entity in entities],
                "relations": [asdict(relation) for relation in relations],
                "entity_type_distribution": entity_type_counts
            },
            f"å®ä½“æ¨æ–­å’Œå…³ç³»æ„å»ºï¼Œç”Ÿæˆ{len(entities)}ä¸ªå®ä½“ï¼Œ{len(relations)}ä¸ªå…³ç³»"
        )

        # 6. åˆ›å»ºçŸ¥è¯†å›¾è°±
        print(f"\nğŸ“¦ æ­¥éª¤6: ç”ŸæˆçŸ¥è¯†å›¾è°±")
        kg = self._create_knowledge_graph(entities, relations, best_schema, document_path, start_time, extraction_result)

        # 7. ä¿å­˜ç»“æœ
        print(f"\nğŸ’¾ æ­¥éª¤7: ä¿å­˜çŸ¥è¯†å›¾è°±")

        # ä¿å­˜æœ€ç»ˆKGåˆ°ä¼šè¯å’Œè¾“å‡ºç›®å½•
        kg_dict = {
            'metadata': kg.metadata,
            'entities': [asdict(entity) for entity in kg.entities],
            'relations': [asdict(relation) for relation in kg.relations],
            'statistics': kg.statistics
        }

        # ä¿å­˜åˆ°ä¼šè¯ç›®å½•
        session_manager.save_stage_result(
            "final_kg",
            kg_dict,
            f"æœ€ç»ˆçŸ¥è¯†å›¾è°±ï¼ŒåŒ…å«{kg.statistics['total_entities']}ä¸ªå®ä½“ï¼Œ{kg.statistics['total_relations']}ä¸ªå…³ç³»"
        )

        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = time.time() - start_time

        # ä¿å­˜åˆ°è¾“å‡ºç›®å½•
        analysis_data = {
            "processing_summary": {
                "total_time": processing_time,
                "schema_used": best_schema,
                "extraction_methods": extraction_result.methods_used,
                "quality_metrics": {
                    "entity_count": kg.statistics['total_entities'],
                    "relation_count": kg.statistics['total_relations'],
                    "avg_entity_confidence": kg.statistics['average_entity_confidence'],
                    "avg_relation_confidence": kg.statistics['average_relation_confidence']
                }
            }
        }

        saved_files = session_manager.save_final_result(kg_dict, analysis_data, best_schema)

        # å…¼å®¹æ—§æ¥å£
        if output_path:
            self._save_knowledge_graph(kg, output_path)
            print(f"   ğŸ“ å…¼å®¹ä¿å­˜: {output_path}")

        # ç»“æŸä¼šè¯
        final_stats = {
            "total_entities": kg.statistics['total_entities'],
            "total_relations": kg.statistics['total_relations'],
            "processing_time": processing_time,
            "schema_used": best_schema,
            "extraction_methods": extraction_result.methods_used if extraction_result else []
        }
        session_manager.end_session(final_stats)

        print(f"\nâœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
        print(f"   â±ï¸ æ€»è€—æ—¶: {processing_time:.2f}s")
        print(f"   ğŸ“Š æœ€ç»ˆç»Ÿè®¡: {kg.statistics['total_entities']} å®ä½“, {kg.statistics['total_relations']} å…³ç³»")
        print(f"   ğŸ“ ä¼šè¯æ–‡ä»¶: {saved_files}")
        print("=" * 80)

        return kg
    
    def _load_document(self, document_path: str) -> str:
        """åŠ è½½æ–‡æ¡£å†…å®¹"""
        path = Path(document_path)
        
        if path.suffix.lower() == '.json':
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('text_content', str(data))
        else:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
    
    def _switch_to_schema(self, schema_file: str):
        """åˆ‡æ¢åˆ°æŒ‡å®šSchema"""
        # å¤„ç†ä¸åŒçš„Schemaè·¯å¾„æ ¼å¼
        if schema_file == "../schemas/general/schema_config.yaml":
            schema_path = "ontology/schemas/general/schema_config.yaml"
        elif schema_file == "spatiotemporal_schema.yaml":
            schema_path = "ontology/schemas/spatiotemporal/spatiotemporal_schema.yaml"
        elif schema_file.startswith("ontology/"):
            schema_path = schema_file
        else:
            schema_path = schema_file

        self.entity_inferer.switch_domain(schema_path)
        self.current_schema = schema_file

        # åˆå§‹åŒ–æ··åˆæŠ½å–å™¨
        self.hybrid_extractor = HybridTripleExtractor(self.entity_inferer.ontology_manager)
    
    def _extract_triples_hybrid(self, text: str):
        """ä½¿ç”¨æ··åˆæ–¹æ³•æŠ½å–ä¸‰å…ƒç»„"""
        if not self.hybrid_extractor:
            raise ValueError("æ··åˆæŠ½å–å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆ‡æ¢Schema")

        # ä½¿ç”¨æ··åˆæŠ½å–å™¨
        extraction_result = self.hybrid_extractor.extract_triples(text)

        # éªŒè¯ä¸‰å…ƒç»„
        validated_triples = []
        for triple in extraction_result.triples:
            if self._validate_triple(triple):
                validated_triples.append(triple)

        # æ›´æ–°ç»“æœ
        extraction_result.triples = validated_triples

        return extraction_result
    
    def _validate_triple(self, triple: Dict[str, Any]) -> bool:
        """éªŒè¯ä¸‰å…ƒç»„çš„æœ‰æ•ˆæ€§"""
        required_fields = ['subject', 'predicate', 'object']

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if field not in triple or not triple[field]:
                print(f"âš ï¸ ä¸‰å…ƒç»„ç¼ºå°‘å­—æ®µ {field}: {triple}")
                return False

        predicate = triple['predicate']
        available_relations = list(self.entity_inferer.ontology_manager.relation_types.keys())

        # æ£€æŸ¥å…³ç³»ç±»å‹æ˜¯å¦åœ¨Schemaä¸­å®šä¹‰
        if predicate not in available_relations:
            # å°è¯•å…³ç³»æ˜ å°„å’Œä¿®å¤
            mapped_predicate = self._map_relation_to_schema(predicate, available_relations)
            if mapped_predicate:
                print(f"ğŸ”„ å…³ç³»æ˜ å°„: '{predicate}' -> '{mapped_predicate}'")
                triple['predicate'] = mapped_predicate
                triple['original_predicate'] = predicate  # ä¿ç•™åŸå§‹å…³ç³»
            else:
                print(f"âŒ å…³ç³»ç±»å‹ '{predicate}' æ— æ³•æ˜ å°„åˆ°Schema")
                print(f"   å¯ç”¨å…³ç³»ç±»å‹: {available_relations[:5]}{'...' if len(available_relations) > 5 else ''}")
                return False

        print(f"âœ… ä¸‰å…ƒç»„éªŒè¯é€šè¿‡: ({triple['subject']}, {triple['predicate']}, {triple['object']})")
        return True

    def _map_relation_to_schema(self, predicate: str, available_relations: List[str]) -> Optional[str]:
        """å°†å…³ç³»æ˜ å°„åˆ°Schemaä¸­çš„åˆæ³•å…³ç³»"""
        predicate_lower = predicate.lower()

        # å…³ç³»æ˜ å°„è§„åˆ™
        relation_mappings = {
            # é€šç”¨æ˜ å°„
            'contains': 'implements',
            'locatedat': 'used_in',
            'located_at': 'used_in',
            'includes': 'implements',
            'has': 'has_algorithm',
            'uses': 'uses_paradigm',

            # æ—¶ç©ºæ˜ å°„
            'hasstarttime': 'hasStartTime',
            'hasendtime': 'hasEndTime',
            'hasduration': 'hasDuration',
            'hascondition': 'hasCondition',
            'resultsin': 'resultsIn',
            'causes': 'resultsIn',
            'triggers': 'resultsIn',

            # å®ä¾‹å…³ç³»æ˜ å°„
            'is_a': 'is_instance_of',
            'isa': 'is_instance_of',
            'instanceof': 'is_instance_of',
            'type_of': 'is_instance_of'
        }

        # ç›´æ¥æ˜ å°„
        if predicate_lower in relation_mappings:
            mapped = relation_mappings[predicate_lower]
            if mapped in available_relations:
                return mapped

        # æ¨¡ç³ŠåŒ¹é…
        for available_rel in available_relations:
            available_lower = available_rel.lower()

            # åŒ…å«åŒ¹é…
            if predicate_lower in available_lower or available_lower in predicate_lower:
                return available_rel

            # ç¼–è¾‘è·ç¦»åŒ¹é…
            if self._calculate_similarity(predicate_lower, available_lower) > 0.7:
                return available_rel

        # é»˜è®¤æ˜ å°„ç­–ç•¥
        if 'is_instance_of' in available_relations:
            return 'is_instance_of'

        return None

    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦"""
        if not str1 or not str2:
            return 0.0

        # ç®€å•çš„ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
        max_len = max(len(str1), len(str2))
        if max_len == 0:
            return 1.0

        # è®¡ç®—å…¬å…±å­—ç¬¦æ•°
        common_chars = sum(1 for c in str1 if c in str2)
        return common_chars / max_len
    
    def _build_entities_and_relations(self, triples: List[Dict[str, Any]],
                                    schema: str, source: str, extraction_result=None) -> Tuple[List[KGEntity], List[KGRelation]]:
        """æ„å»ºå®ä½“å’Œå…³ç³»å¯¹è±¡"""
        entities = []
        relations = []
        entity_names = set()
        
        current_time = datetime.now().isoformat()
        
        # æ”¶é›†æ‰€æœ‰å®ä½“åç§°
        for triple in triples:
            entity_names.add(triple['subject'])
            entity_names.add(triple['object'])
        
        # åˆ›å»ºå®ä½“å¯¹è±¡
        for entity_name in entity_names:
            entity_id = self._generate_entity_id(entity_name)
            
            # æ¨æ–­å®ä½“ç±»å‹
            inference_result = self.entity_inferer.infer_entity_type(entity_name)
            entity_type = inference_result.entity_type or "Unknown"
            
            # åˆ›å»ºå®ä½“å±æ€§
            properties = {
                'inference_method': inference_result.method,
                'inference_time': inference_result.inference_time,
                'original_name': entity_name
            }
            
            # æ·»åŠ Schemaç‰¹å®šå±æ€§
            if entity_type in self.entity_inferer.ontology_manager.entity_types:
                type_config = self.entity_inferer.ontology_manager.entity_types[entity_type]
                if hasattr(type_config, 'description'):
                    properties['type_description'] = type_config.description
                if hasattr(type_config, 'aliases'):
                    properties['aliases'] = type_config.aliases
            
            entity = KGEntity(
                id=entity_id,
                name=entity_name,
                type=entity_type,
                properties=properties,
                confidence=inference_result.confidence,
                source=source,
                created_at=current_time,
                schema=schema
            )
            
            entities.append(entity)
            self.entity_id_map[entity_name] = entity_id
        
        # åˆ›å»ºå…³ç³»å¯¹è±¡
        for i, triple in enumerate(triples):
            relation_id = f"rel_{uuid.uuid4().hex[:8]}"
            
            subject_id = self.entity_id_map[triple['subject']]
            object_id = self.entity_id_map[triple['object']]
            
            # åˆ›å»ºå…³ç³»å±æ€§
            properties = {
                'extraction_confidence': triple.get('confidence', 0.8),
                'triple_index': i
            }
            
            # æ·»åŠ Schemaç‰¹å®šå±æ€§
            predicate = triple['predicate']
            if predicate in self.entity_inferer.ontology_manager.relation_types:
                rel_config = self.entity_inferer.ontology_manager.relation_types[predicate]
                if hasattr(rel_config, 'description'):
                    properties['relation_description'] = rel_config.description
                if hasattr(rel_config, 'subject_types'):
                    properties['expected_subject_types'] = rel_config.subject_types
                if hasattr(rel_config, 'object_types'):
                    properties['expected_object_types'] = rel_config.object_types
            
            relation = KGRelation(
                id=relation_id,
                subject_id=subject_id,
                subject_name=triple['subject'],
                predicate=predicate,
                object_id=object_id,
                object_name=triple['object'],
                properties=properties,
                confidence=triple.get('confidence', 0.8),
                source=source,
                created_at=current_time,
                schema=schema
            )
            
            relations.append(relation)
        
        return entities, relations
    
    def _generate_entity_id(self, entity_name: str) -> str:
        """ç”Ÿæˆå®ä½“ID"""
        # ä½¿ç”¨å®ä½“åç§°çš„hashç”Ÿæˆç¨³å®šçš„ID
        import hashlib
        name_hash = hashlib.md5(entity_name.encode('utf-8')).hexdigest()[:8]
        return f"entity_{name_hash}"
    
    def _create_knowledge_graph(self, entities: List[KGEntity], relations: List[KGRelation],
                              schema: str, source: str, start_time: float, extraction_result=None) -> KnowledgeGraph:
        """åˆ›å»ºå®Œæ•´çš„çŸ¥è¯†å›¾è°±å¯¹è±¡"""
        processing_time = time.time() - start_time
        
        # ç»Ÿè®¡ä¿¡æ¯
        entity_type_counts = {}
        relation_type_counts = {}
        
        for entity in entities:
            entity_type_counts[entity.type] = entity_type_counts.get(entity.type, 0) + 1
        
        for relation in relations:
            relation_type_counts[relation.predicate] = relation_type_counts.get(relation.predicate, 0) + 1
        
        # å…ƒæ•°æ®
        metadata = {
            'schema': schema,
            'source_document': source,
            'created_at': datetime.now().isoformat(),
            'processing_time': processing_time,
            'builder_version': '3.0.0'
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        statistics = {
            'total_entities': len(entities),
            'total_relations': len(relations),
            'entity_type_distribution': entity_type_counts,
            'relation_type_distribution': relation_type_counts,
            'average_entity_confidence': sum(e.confidence for e in entities) / len(entities) if entities else 0,
            'average_relation_confidence': sum(r.confidence for r in relations) / len(relations) if relations else 0
        }

        # æ·»åŠ æŠ½å–ç»Ÿè®¡ä¿¡æ¯
        if extraction_result:
            extraction_stats = self.hybrid_extractor.get_extraction_statistics(extraction_result)
            statistics.update({
                'extraction_method_breakdown': extraction_stats['method_breakdown'],
                'rule_extraction_count': extraction_stats['rule_triples'],
                'llm_extraction_count': extraction_stats['llm_triples'],
                'extraction_methods_used': extraction_stats['methods_used'],
                'triple_extraction_time': extraction_stats['processing_time']
            })
        
        return KnowledgeGraph(
            metadata=metadata,
            entities=entities,
            relations=relations,
            statistics=statistics
        )
    
    def _save_knowledge_graph(self, kg: KnowledgeGraph, output_path: str):
        """ä¿å­˜çŸ¥è¯†å›¾è°±åˆ°JSONæ–‡ä»¶"""
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        kg_dict = {
            'metadata': kg.metadata,
            'entities': [asdict(entity) for entity in kg.entities],
            'relations': [asdict(relation) for relation in kg.relations],
            'statistics': kg.statistics
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(kg_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ°: {output_path}")

# å…¨å±€æ„å»ºå™¨å®ä¾‹
kg_builder = SchemaBasedKGBuilder()
