#!/usr/bin/env python3
"""
å®ä½“ç±»å‹ç¼“å­˜ç®¡ç†å·¥å…·
ç”¨äºç®¡ç†å’Œå®¡æ ¸å®ä½“ç±»å‹æ¨æ–­ç¼“å­˜
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import argparse
from datetime import datetime
from pipeline.enhanced_entity_inferer import EnhancedEntityTypeInferer
from pipeline.stage_saver import StageSaver

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.inferer = EnhancedEntityTypeInferer()
        self.stage_saver = StageSaver()
    
    def show_cache_stats(self):
        """æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.inferer.get_cache_stats()
        
        print("ğŸ“Š å®ä½“ç±»å‹ç¼“å­˜ç»Ÿè®¡")
        print("=" * 50)
        print(f"æ€»ç¼“å­˜å®ä½“æ•°: {stats['total_cached_entities']}")
        print(f"é«˜é¢‘å®ä½“æ•°: {stats['high_frequency_entities']}")
        print(f"ç¼“å­˜å‘½ä¸­æ½œåŠ›: {stats['cache_hit_potential']:.2%}")
        print()
        
        # æ˜¾ç¤ºç¼“å­˜è¯¦æƒ…
        if stats['total_cached_entities'] > 0:
            print("ğŸ“‹ ç¼“å­˜è¯¦æƒ…:")
            for entity, info in list(self.inferer.entity_cache.verified_entities.items())[:10]:
                print(f"  {entity:<20} | {info['type']:<12} | ä½¿ç”¨{info['count']}æ¬¡ | {info['source']}")
            
            if stats['total_cached_entities'] > 10:
                print(f"  ... è¿˜æœ‰ {stats['total_cached_entities'] - 10} ä¸ªå®ä½“")
    
    def export_for_review(self, output_file: str = None):
        """å¯¼å‡ºé«˜é¢‘å®ä½“ä¾›äººå·¥å®¡æ ¸"""
        high_freq_entities = self.inferer.entity_cache.export_for_review()
        
        if not high_freq_entities:
            print("ğŸ“­ æ²¡æœ‰é«˜é¢‘å®ä½“éœ€è¦å®¡æ ¸")
            return
        
        # åˆ›å»ºå®¡æ ¸æ–‡ä»¶
        review_file = self.stage_saver.create_manual_review_file(
            "é«˜é¢‘å®ä½“ç±»å‹å®¡æ ¸",
            high_freq_entities
        )
        
        print(f"ğŸ“‹ é«˜é¢‘å®ä½“å®¡æ ¸æ–‡ä»¶å·²åˆ›å»º: {review_file}")
        print(f"ğŸ“ åŒ…å« {len(high_freq_entities)} ä¸ªé«˜é¢‘å®ä½“")
        print("ğŸ“ è¯·ç¼–è¾‘æ–‡ä»¶ï¼Œè®¾ç½® approved å­—æ®µä¸º true/false")
        
        return review_file
    
    def import_reviewed_entities(self, review_file: str):
        """å¯¼å…¥å®¡æ ¸åçš„å®ä½“ç±»å‹"""
        try:
            with open(review_file, 'r', encoding='utf-8') as f:
                review_data = json.load(f)
            
            approved_count = 0
            rejected_count = 0
            
            for item in review_data.get("items", []):
                if item.get("approved") is True:
                    entity_data = item["data"]
                    self.inferer.entity_cache.add_verified_entity(
                        entity_data["entity"],
                        entity_data["type"],
                        1.0,  # äººå·¥å®¡æ ¸çš„ç½®ä¿¡åº¦è®¾ä¸º1.0
                        "human_approved"
                    )
                    approved_count += 1
                elif item.get("approved") is False:
                    rejected_count += 1
            
            print(f"âœ… å¯¼å…¥å®Œæˆ: {approved_count} ä¸ªå®ä½“é€šè¿‡å®¡æ ¸, {rejected_count} ä¸ªè¢«æ‹’ç»")
            
        except Exception as e:
            print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    
    def clean_cache(self, min_confidence: float = 0.5, min_usage: int = 1):
        """æ¸…ç†ä½è´¨é‡ç¼“å­˜é¡¹"""
        original_count = len(self.inferer.entity_cache.verified_entities)
        
        # è¿‡æ»¤ä½è´¨é‡é¡¹
        filtered_entities = {}
        for entity, info in self.inferer.entity_cache.verified_entities.items():
            if info['confidence'] >= min_confidence and info['count'] >= min_usage:
                filtered_entities[entity] = info
        
        self.inferer.entity_cache.verified_entities = filtered_entities
        cleaned_count = original_count - len(filtered_entities)
        
        print(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ: ç§»é™¤äº† {cleaned_count} ä¸ªä½è´¨é‡é¡¹")
        print(f"ğŸ“Š å‰©ä½™ç¼“å­˜é¡¹: {len(filtered_entities)}")
    
    def backup_cache(self, backup_file: str = None):
        """å¤‡ä»½ç¼“å­˜"""
        if not backup_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = f"cache_backup_{timestamp}.json"
        
        backup_data = {
            "backup_time": datetime.now().isoformat(),
            "total_entities": len(self.inferer.entity_cache.verified_entities),
            "entities": self.inferer.entity_cache.verified_entities
        }
        
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ç¼“å­˜å·²å¤‡ä»½åˆ°: {backup_file}")
        return backup_file
    
    def restore_cache(self, backup_file: str):
        """æ¢å¤ç¼“å­˜"""
        try:
            with open(backup_file, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)
            
            self.inferer.entity_cache.verified_entities = backup_data["entities"]
            
            print(f"ğŸ”„ ç¼“å­˜å·²æ¢å¤: {backup_data['total_entities']} ä¸ªå®ä½“")
            print(f"ğŸ“… å¤‡ä»½æ—¶é—´: {backup_data['backup_time']}")
            
        except Exception as e:
            print(f"âŒ æ¢å¤å¤±è´¥: {e}")
    
    def analyze_cache_performance(self):
        """åˆ†æç¼“å­˜æ€§èƒ½"""
        entities = self.inferer.entity_cache.verified_entities
        
        if not entities:
            print("ğŸ“­ ç¼“å­˜ä¸ºç©ºï¼Œæ— æ³•åˆ†æ")
            return
        
        # æŒ‰æ¥æºåˆ†ç»„ç»Ÿè®¡
        source_stats = {}
        confidence_stats = []
        usage_stats = []
        
        for info in entities.values():
            source = info['source']
            source_stats[source] = source_stats.get(source, 0) + 1
            confidence_stats.append(info['confidence'])
            usage_stats.append(info['count'])
        
        print("ğŸ“ˆ ç¼“å­˜æ€§èƒ½åˆ†æ")
        print("=" * 50)
        
        print("ğŸ“Š æŒ‰æ¥æºç»Ÿè®¡:")
        for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(entities) * 100
            print(f"  {source:<25} | {count:>4} ä¸ª ({percentage:>5.1f}%)")
        
        print(f"\nğŸ“Š ç½®ä¿¡åº¦ç»Ÿè®¡:")
        avg_confidence = sum(confidence_stats) / len(confidence_stats)
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        print(f"  æœ€é«˜ç½®ä¿¡åº¦: {max(confidence_stats):.3f}")
        print(f"  æœ€ä½ç½®ä¿¡åº¦: {min(confidence_stats):.3f}")
        
        print(f"\nğŸ“Š ä½¿ç”¨é¢‘ç‡ç»Ÿè®¡:")
        avg_usage = sum(usage_stats) / len(usage_stats)
        print(f"  å¹³å‡ä½¿ç”¨æ¬¡æ•°: {avg_usage:.1f}")
        print(f"  æœ€é«˜ä½¿ç”¨æ¬¡æ•°: {max(usage_stats)}")
        print(f"  å•æ¬¡ä½¿ç”¨å®ä½“: {sum(1 for u in usage_stats if u == 1)}")

def main():
    parser = argparse.ArgumentParser(description="å®ä½“ç±»å‹ç¼“å­˜ç®¡ç†å·¥å…·")
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    subparsers.add_parser('stats', help='æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯')
    
    # å¯¼å‡ºå®¡æ ¸æ–‡ä»¶
    export_parser = subparsers.add_parser('export', help='å¯¼å‡ºé«˜é¢‘å®ä½“ä¾›å®¡æ ¸')
    export_parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    # å¯¼å…¥å®¡æ ¸ç»“æœ
    import_parser = subparsers.add_parser('import', help='å¯¼å…¥å®¡æ ¸åçš„å®ä½“ç±»å‹')
    import_parser.add_argument('review_file', help='å®¡æ ¸æ–‡ä»¶è·¯å¾„')
    
    # æ¸…ç†ç¼“å­˜
    clean_parser = subparsers.add_parser('clean', help='æ¸…ç†ä½è´¨é‡ç¼“å­˜é¡¹')
    clean_parser.add_argument('--min-confidence', type=float, default=0.5, help='æœ€ä½ç½®ä¿¡åº¦')
    clean_parser.add_argument('--min-usage', type=int, default=1, help='æœ€ä½ä½¿ç”¨æ¬¡æ•°')
    
    # å¤‡ä»½ç¼“å­˜
    backup_parser = subparsers.add_parser('backup', help='å¤‡ä»½ç¼“å­˜')
    backup_parser.add_argument('--output', help='å¤‡ä»½æ–‡ä»¶è·¯å¾„')
    
    # æ¢å¤ç¼“å­˜
    restore_parser = subparsers.add_parser('restore', help='æ¢å¤ç¼“å­˜')
    restore_parser.add_argument('backup_file', help='å¤‡ä»½æ–‡ä»¶è·¯å¾„')
    
    # æ€§èƒ½åˆ†æ
    subparsers.add_parser('analyze', help='åˆ†æç¼“å­˜æ€§èƒ½')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    manager = CacheManager()
    
    if args.command == 'stats':
        manager.show_cache_stats()
    elif args.command == 'export':
        manager.export_for_review(args.output)
    elif args.command == 'import':
        manager.import_reviewed_entities(args.review_file)
    elif args.command == 'clean':
        manager.clean_cache(args.min_confidence, args.min_usage)
    elif args.command == 'backup':
        manager.backup_cache(args.output)
    elif args.command == 'restore':
        manager.restore_cache(args.backup_file)
    elif args.command == 'analyze':
        manager.analyze_cache_performance()

if __name__ == "__main__":
    main()
