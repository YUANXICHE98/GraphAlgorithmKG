#!/usr/bin/env python3
"""
å½»åº•æ¸…ç†resultsç›®å½•
æ¶ˆé™¤é‡å¤æ–‡ä»¶ï¼Œè§„èŒƒç»“æ„ï¼Œæ·»åŠ CSVè¾“å‡º
"""

import os
import json
import shutil
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

def cleanup_results_directory():
    """å½»åº•æ¸…ç†resultsç›®å½•"""
    
    print("ğŸ§¹ å¼€å§‹å½»åº•æ¸…ç†resultsç›®å½•")
    print("ğŸ¯ ç›®æ ‡: æ¶ˆé™¤é‡å¤ï¼Œè§„èŒƒç»“æ„ï¼Œæ·»åŠ å¤šæ ¼å¼è¾“å‡º")
    print("=" * 80)
    
    results_path = Path("results")
    
    # 1. å¤‡ä»½å½“å‰resultsç›®å½•
    backup_path = Path(f"results_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    if results_path.exists():
        shutil.copytree(results_path, backup_path)
        print(f"ğŸ’¾ å·²å¤‡ä»½åˆ°: {backup_path}")
    
    # 2. åˆ›å»ºæ–°çš„æ¸…æ™°ç»“æ„
    new_structure = {
        "sessions": "å®Œæ•´çš„å¤„ç†ä¼šè¯è®°å½•",
        "knowledge_graphs": {
            "general": "é€šç”¨çŸ¥è¯†å›¾è°±",
            "spatiotemporal": "æ—¶ç©ºçŸ¥è¯†å›¾è°±", 
            "mixed": "æ··åˆç±»å‹çŸ¥è¯†å›¾è°±"
        },
        "exports": {
            "json": "JSONæ ¼å¼å¯¼å‡º",
            "csv": "CSVæ ¼å¼å¯¼å‡º",
            "graphml": "GraphMLæ ¼å¼å¯¼å‡º",
            "gexf": "Gephiæ ¼å¼å¯¼å‡º"
        },
        "analysis": "è´¨é‡åˆ†æå’Œç»Ÿè®¡æŠ¥å‘Š",
        "cache": {
            "entities": "å®ä½“æ¨æ–­ç¼“å­˜",
            "schemas": "Schemaæ£€æµ‹ç¼“å­˜"
        },
        "backups": "å†å²å¤‡ä»½æ–‡ä»¶"
    }
    
    print("ğŸ“ åˆ›å»ºæ–°çš„ç›®å½•ç»“æ„:")
    create_directory_structure(results_path, new_structure)
    
    # 3. æ•´ç†å’Œå»é‡çŸ¥è¯†å›¾è°±æ–‡ä»¶
    print(f"\nğŸ“¦ æ•´ç†çŸ¥è¯†å›¾è°±æ–‡ä»¶:")
    consolidate_knowledge_graphs(results_path)
    
    # 4. ç”Ÿæˆå¤šæ ¼å¼å¯¼å‡º
    print(f"\nğŸ“¤ ç”Ÿæˆå¤šæ ¼å¼å¯¼å‡º:")
    generate_multi_format_exports(results_path)
    
    # 5. æ¸…ç†é‡å¤å’Œæ— ç”¨æ–‡ä»¶
    print(f"\nğŸ—‘ï¸ æ¸…ç†é‡å¤å’Œæ— ç”¨æ–‡ä»¶:")
    cleanup_duplicates_and_unused(results_path)
    
    # 6. ç”Ÿæˆç›®å½•ç»“æ„æŠ¥å‘Š
    print(f"\nğŸ“Š ç”Ÿæˆæœ€ç»ˆç»“æ„æŠ¥å‘Š:")
    generate_structure_report(results_path)
    
    print(f"\nâœ… resultsç›®å½•æ¸…ç†å®Œæˆ!")

def create_directory_structure(base_path: Path, structure: Dict[str, Any], prefix: str = ""):
    """é€’å½’åˆ›å»ºç›®å½•ç»“æ„"""
    for key, value in structure.items():
        if isinstance(value, dict):
            # åˆ›å»ºå­ç›®å½•
            dir_path = base_path / key
            dir_path.mkdir(parents=True, exist_ok=True)
            print(f"   âœ… {prefix}{key}/")
            
            # é€’å½’åˆ›å»ºå­ç»“æ„
            create_directory_structure(dir_path, value, prefix + "  ")
        else:
            # åˆ›å»ºç›®å½•å¹¶æ·»åŠ è¯´æ˜
            dir_path = base_path / key
            dir_path.mkdir(parents=True, exist_ok=True)
            print(f"   âœ… {prefix}{key}: {value}")

def consolidate_knowledge_graphs(results_path: Path):
    """æ•´ç†å’Œå»é‡çŸ¥è¯†å›¾è°±æ–‡ä»¶"""
    
    # æ”¶é›†æ‰€æœ‰KGæ–‡ä»¶
    kg_files = []
    
    # ä»åŸå§‹outputsç›®å½•æ”¶é›†
    old_kg_path = results_path / "outputs" / "knowledge_graphs"
    if old_kg_path.exists():
        kg_files.extend(old_kg_path.glob("*.json"))
    
    # ä»by_schemaç›®å½•æ”¶é›†
    by_schema_path = results_path / "outputs" / "by_schema"
    if by_schema_path.exists():
        for schema_dir in by_schema_path.iterdir():
            if schema_dir.is_dir():
                kg_dir = schema_dir / "knowledge_graphs"
                if kg_dir.exists():
                    kg_files.extend(kg_dir.glob("*.json"))
    
    print(f"   ğŸ“„ æ‰¾åˆ° {len(kg_files)} ä¸ªKGæ–‡ä»¶")
    
    # æŒ‰Schemaåˆ†ç±»å¹¶å»é‡
    schema_files = {"general": [], "spatiotemporal": [], "mixed": []}
    processed_files = set()
    
    for kg_file in kg_files:
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(kg_file, 'r', encoding='utf-8') as f:
                kg_data = json.load(f)
            
            # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡
            content_hash = hash(json.dumps(kg_data, sort_keys=True))
            if content_hash in processed_files:
                print(f"   ğŸ”„ è·³è¿‡é‡å¤æ–‡ä»¶: {kg_file.name}")
                continue
            
            processed_files.add(content_hash)
            
            # ç¡®å®šSchemaç±»å‹
            schema_used = kg_data.get('metadata', {}).get('schema', '')
            schema_category = classify_schema_type(schema_used)
            
            # ç”Ÿæˆæ ‡å‡†æ–‡ä»¶å
            session_id = extract_session_id_from_filename(kg_file.name)
            new_filename = f"{session_id}_{schema_category}_kg.json"
            
            # ç§»åŠ¨åˆ°æ–°ä½ç½®
            target_path = results_path / "knowledge_graphs" / schema_category / new_filename
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ–‡ä»¶
            with open(target_path, 'w', encoding='utf-8') as f:
                json.dump(kg_data, f, ensure_ascii=False, indent=2)
            
            schema_files[schema_category].append(target_path)
            print(f"   ğŸ“„ {kg_file.name} -> {schema_category}/{new_filename}")
            
        except Exception as e:
            print(f"   âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {kg_file.name}: {e}")
    
    # ç»Ÿè®¡ç»“æœ
    for schema, files in schema_files.items():
        print(f"   ğŸ“Š {schema}: {len(files)} ä¸ªæ–‡ä»¶")

def generate_multi_format_exports(results_path: Path):
    """ç”Ÿæˆå¤šæ ¼å¼å¯¼å‡ºæ–‡ä»¶"""
    
    # ä¸ºæ¯ä¸ªSchemaç±»å‹ç”Ÿæˆå¯¼å‡º
    for schema_type in ["general", "spatiotemporal", "mixed"]:
        kg_dir = results_path / "knowledge_graphs" / schema_type
        if not kg_dir.exists():
            continue
        
        kg_files = list(kg_dir.glob("*.json"))
        if not kg_files:
            continue
        
        print(f"   ğŸ“Š å¤„ç† {schema_type} Schema ({len(kg_files)} ä¸ªæ–‡ä»¶)")
        
        # åˆå¹¶æ‰€æœ‰KGæ•°æ®
        all_entities = []
        all_relations = []
        
        for kg_file in kg_files:
            try:
                with open(kg_file, 'r', encoding='utf-8') as f:
                    kg_data = json.load(f)
                
                # æå–å®ä½“
                for entity in kg_data.get('entities', []):
                    entity_record = {
                        'id': entity.get('id', ''),
                        'name': entity.get('name', ''),
                        'type': entity.get('type', ''),
                        'schema': schema_type,
                        'confidence': entity.get('confidence', 0.0),
                        'source_file': kg_file.name
                    }
                    all_entities.append(entity_record)
                
                # æå–å…³ç³»
                for relation in kg_data.get('relations', []):
                    relation_record = {
                        'id': relation.get('id', ''),
                        'subject': relation.get('subject', ''),
                        'predicate': relation.get('predicate', ''),
                        'object': relation.get('object', ''),
                        'schema': schema_type,
                        'confidence': relation.get('confidence', 0.0),
                        'source_file': kg_file.name
                    }
                    all_relations.append(relation_record)
                    
            except Exception as e:
                print(f"     âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {kg_file.name}: {e}")
        
        # ç”ŸæˆCSVå¯¼å‡º
        if all_entities:
            entities_df = pd.DataFrame(all_entities)
            entities_csv = results_path / "exports" / "csv" / f"{schema_type}_entities.csv"
            entities_csv.parent.mkdir(parents=True, exist_ok=True)
            entities_df.to_csv(entities_csv, index=False, encoding='utf-8')
            print(f"     ğŸ“„ å®ä½“CSV: {entities_csv.name} ({len(all_entities)} è¡Œ)")
        
        if all_relations:
            relations_df = pd.DataFrame(all_relations)
            relations_csv = results_path / "exports" / "csv" / f"{schema_type}_relations.csv"
            relations_df.to_csv(relations_csv, index=False, encoding='utf-8')
            print(f"     ğŸ“„ å…³ç³»CSV: {relations_csv.name} ({len(all_relations)} è¡Œ)")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        stats = {
            'schema_type': schema_type,
            'total_files': len(kg_files),
            'total_entities': len(all_entities),
            'total_relations': len(all_relations),
            'entity_types': list(set(e['type'] for e in all_entities)),
            'relation_types': list(set(r['predicate'] for r in all_relations)),
            'generated_time': datetime.now().isoformat()
        }
        
        stats_file = results_path / "analysis" / f"{schema_type}_statistics.json"
        stats_file.parent.mkdir(parents=True, exist_ok=True)
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"     ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š: {stats_file.name}")

def cleanup_duplicates_and_unused(results_path: Path):
    """æ¸…ç†é‡å¤å’Œæ— ç”¨æ–‡ä»¶"""
    
    # åˆ é™¤æ—§çš„outputsç›®å½•
    old_outputs = results_path / "outputs"
    if old_outputs.exists():
        shutil.rmtree(old_outputs)
        print(f"   ğŸ—‘ï¸ åˆ é™¤æ—§çš„outputsç›®å½•")
    
    # æ¸…ç†ç©ºç›®å½•
    for root, dirs, files in os.walk(results_path, topdown=False):
        for dir_name in dirs:
            dir_path = Path(root) / dir_name
            try:
                if not any(dir_path.iterdir()):
                    dir_path.rmdir()
                    print(f"   ğŸ—‘ï¸ åˆ é™¤ç©ºç›®å½•: {dir_path.relative_to(results_path)}")
            except OSError:
                pass  # ç›®å½•ä¸ä¸ºç©ºæˆ–å…¶ä»–é”™è¯¯
    
    # ç§»åŠ¨å¤‡ä»½æ–‡ä»¶
    backups_dir = results_path / "backups"
    backups_dir.mkdir(exist_ok=True)
    
    for backup_file in results_path.glob("*.pkl"):
        target_path = backups_dir / backup_file.name
        shutil.move(str(backup_file), str(target_path))
        print(f"   ğŸ“¦ ç§»åŠ¨å¤‡ä»½æ–‡ä»¶: {backup_file.name}")

def generate_structure_report(results_path: Path):
    """ç”Ÿæˆæœ€ç»ˆç»“æ„æŠ¥å‘Š"""
    
    report = {
        'cleanup_time': datetime.now().isoformat(),
        'directory_structure': {},
        'file_statistics': {},
        'schema_distribution': {}
    }
    
    # ç»Ÿè®¡ç›®å½•ç»“æ„
    for item in results_path.rglob('*'):
        if item.is_file():
            rel_path = item.relative_to(results_path)
            category = str(rel_path.parts[0]) if rel_path.parts else 'root'
            
            if category not in report['file_statistics']:
                report['file_statistics'][category] = {'count': 0, 'total_size': 0}
            
            report['file_statistics'][category]['count'] += 1
            report['file_statistics'][category]['total_size'] += item.stat().st_size
    
    # ç»Ÿè®¡Schemaåˆ†å¸ƒ
    kg_base = results_path / "knowledge_graphs"
    if kg_base.exists():
        for schema_dir in kg_base.iterdir():
            if schema_dir.is_dir():
                kg_files = list(schema_dir.glob("*.json"))
                report['schema_distribution'][schema_dir.name] = len(kg_files)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = results_path / "CLEANUP_REPORT.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æ‘˜è¦
    print("ğŸ“Š æœ€ç»ˆç»“æ„æ‘˜è¦:")
    for category, stats in report['file_statistics'].items():
        size_mb = stats['total_size'] / (1024 * 1024)
        print(f"   ğŸ“ {category}: {stats['count']} æ–‡ä»¶, {size_mb:.1f} MB")
    
    print("ğŸ“‹ Schemaåˆ†å¸ƒ:")
    for schema, count in report['schema_distribution'].items():
        print(f"   ğŸ¯ {schema}: {count} ä¸ªçŸ¥è¯†å›¾è°±")
    
    print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")

def classify_schema_type(schema_path: str) -> str:
    """æ ¹æ®schemaè·¯å¾„åˆ†ç±»"""
    if not schema_path:
        return "mixed"
    
    schema_path_lower = schema_path.lower()
    
    if "schema_config" in schema_path_lower or "general" in schema_path_lower:
        return "general"
    elif "spatiotemporal" in schema_path_lower or "spatial" in schema_path_lower:
        return "spatiotemporal"
    else:
        return "mixed"

def extract_session_id_from_filename(filename: str) -> str:
    """ä»æ–‡ä»¶åæå–ä¼šè¯ID"""
    # å°è¯•æå–æ—¶é—´æˆ³æ ¼å¼çš„ä¼šè¯ID
    parts = filename.split('_')
    if len(parts) >= 2 and parts[0].isdigit() and parts[1].isdigit():
        return f"{parts[0]}_{parts[1]}"
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨æ–‡ä»¶åå‰ç¼€
    return filename.split('.')[0].split('_')[0]

if __name__ == "__main__":
    cleanup_results_directory()
