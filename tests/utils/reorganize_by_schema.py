#!/usr/bin/env python3
"""
æŒ‰Schemaé‡æ–°ç»„ç»‡è¾“å‡ºç»“æ„
ç¡®ä¿è¾“å…¥æ··ä¹±ï¼Œè¾“å‡ºè§„èŒƒï¼Œè¾¹ç•Œæ¸…æ™°
"""

import os
import json
import shutil
from pathlib import Path
from datetime import datetime

def reorganize_by_schema():
    """æŒ‰Schemaé‡æ–°ç»„ç»‡è¾“å‡ºç»“æ„"""
    
    print("ğŸ¯ æŒ‰Schemaé‡æ–°ç»„ç»‡è¾“å‡ºç»“æ„")
    print("ğŸ“‹ ç›®æ ‡: è¾“å…¥æ··ä¹±ï¼Œè¾“å‡ºè§„èŒƒï¼Œè¾¹ç•Œæ¸…æ™°")
    print("=" * 80)
    
    results_path = Path("results")
    
    # 1. æ–°çš„Schemaåˆ†ç±»ç»“æ„
    schema_structure = {
        "general": {
            "name": "é€šç”¨çŸ¥è¯†å›¾è°±",
            "schema_files": ["schema_config.yaml", "../schema_config.yaml"],
            "description": "ä¼ä¸šæ¶æ„ã€æ¡†æ¶ã€ç®—æ³•ç­‰é€šç”¨æ¦‚å¿µ"
        },
        "spatiotemporal": {
            "name": "æ—¶ç©ºçŸ¥è¯†å›¾è°±", 
            "schema_files": ["spatiotemporal_schema.yaml"],
            "description": "æ—¶ç©ºæ¦‚å¿µã€äº‹ä»¶ã€DO-DA-Fç»“æ„"
        },
        "mixed": {
            "name": "æ··åˆç±»å‹",
            "schema_files": [],
            "description": "æ— æ³•æ˜ç¡®åˆ†ç±»æˆ–å¤šSchemaæ··åˆçš„ç»“æœ"
        }
    }
    
    print("ğŸ“ åˆ›å»ºæŒ‰Schemaåˆ†ç±»çš„ç›®å½•ç»“æ„:")
    
    # åˆ›å»ºæ–°çš„ç›®å½•ç»“æ„
    for schema_key, schema_info in schema_structure.items():
        schema_dir = results_path / "outputs" / "by_schema" / schema_key
        
        # åˆ›å»ºå­ç›®å½•
        subdirs = [
            "knowledge_graphs",  # æœ€ç»ˆKGæ–‡ä»¶
            "sessions",         # è¯¥Schemaçš„ä¼šè¯è®°å½•
            "analysis",         # åˆ†ææŠ¥å‘Š
            "exports",          # å¯¼å‡ºæ–‡ä»¶
            "statistics"        # ç»Ÿè®¡ä¿¡æ¯
        ]
        
        for subdir in subdirs:
            (schema_dir / subdir).mkdir(parents=True, exist_ok=True)
        
        print(f"   âœ… {schema_key}: {schema_info['name']}")
        print(f"      ğŸ“ {schema_info['description']}")
        
        # åˆ›å»ºSchemaè¯´æ˜æ–‡ä»¶
        schema_readme = schema_dir / "README.md"
        readme_content = f"""# {schema_info['name']}

## ğŸ“‹ Schemaæè¿°
{schema_info['description']}

## ğŸ“ ç›®å½•ç»“æ„
- `knowledge_graphs/`: æœ€ç»ˆçš„çŸ¥è¯†å›¾è°±JSONæ–‡ä»¶
- `sessions/`: è¯¥Schemaçš„å¤„ç†ä¼šè¯è®°å½•
- `analysis/`: è´¨é‡åˆ†æå’Œç»Ÿè®¡æŠ¥å‘Š
- `exports/`: å„ç§æ ¼å¼çš„å¯¼å‡ºæ–‡ä»¶
- `statistics/`: æ€§èƒ½å’Œè´¨é‡ç»Ÿè®¡

## ğŸ¯ è¾“å‡ºè§„èŒƒ
- æ‰€æœ‰å®ä½“ç±»å‹å¿…é¡»ç¬¦åˆå¯¹åº”Schemaå®šä¹‰
- æ‰€æœ‰å…³ç³»ç±»å‹å¿…é¡»åœ¨Schemaå…³ç³»åˆ—è¡¨ä¸­
- æ–‡ä»¶å‘½åæ ¼å¼: `{{session_id}}_{{schema_type}}_{{file_type}}.{{ext}}`

## ğŸ“Š è´¨é‡è¦æ±‚
- å®ä½“æ¨æ–­å‡†ç¡®ç‡ > 80%
- å…³ç³»æŠ½å–å‡†ç¡®ç‡ > 75%
- Schemaä¸€è‡´æ€§ = 100%
"""
        
        with open(schema_readme, 'w', encoding='utf-8') as f:
            f.write(readme_content)
    
    # 2. åˆ†æç°æœ‰æ–‡ä»¶å¹¶é‡æ–°åˆ†ç±»
    print(f"\nğŸ“¦ åˆ†æå’Œé‡æ–°åˆ†ç±»ç°æœ‰æ–‡ä»¶:")
    
    # åˆ†æçŸ¥è¯†å›¾è°±æ–‡ä»¶
    kg_path = results_path / "outputs" / "knowledge_graphs"
    if kg_path.exists():
        kg_files = list(kg_path.glob("*.json"))
        
        for kg_file in kg_files:
            try:
                with open(kg_file, 'r', encoding='utf-8') as f:
                    kg_data = json.load(f)
                
                # æ ¹æ®metadataä¸­çš„schemaåˆ¤æ–­åˆ†ç±»
                schema_used = kg_data.get('metadata', {}).get('schema', '')
                schema_category = classify_schema(schema_used)
                
                # ç”Ÿæˆæ–°çš„æ–‡ä»¶å
                session_id = extract_session_id(kg_file.name)
                new_filename = f"{session_id}_{schema_category}_knowledge_graph.json"
                
                # ç§»åŠ¨åˆ°å¯¹åº”ç›®å½•
                target_dir = results_path / "outputs" / "by_schema" / schema_category / "knowledge_graphs"
                target_file = target_dir / new_filename
                
                shutil.copy2(kg_file, target_file)
                print(f"   ğŸ“„ {kg_file.name} -> {schema_category}/{new_filename}")
                
            except Exception as e:
                print(f"   âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {kg_file.name}: {e}")
    
    # åˆ†æä¼šè¯æ–‡ä»¶
    sessions_path = results_path / "sessions"
    if sessions_path.exists():
        for session_dir in sessions_path.iterdir():
            if session_dir.is_dir() and session_dir.name != "latest":
                try:
                    # è¯»å–ä¼šè¯å…ƒæ•°æ®
                    metadata_file = session_dir / "session_metadata.json"
                    if metadata_file.exists():
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            session_data = json.load(f)
                        
                        schema_used = session_data.get('schema_used', '')
                        schema_category = classify_schema(schema_used)
                        
                        # å¤åˆ¶æ•´ä¸ªä¼šè¯ç›®å½•
                        target_session_dir = results_path / "outputs" / "by_schema" / schema_category / "sessions" / session_dir.name
                        
                        if not target_session_dir.exists():
                            shutil.copytree(session_dir, target_session_dir)
                            print(f"   ğŸ“ ä¼šè¯ {session_dir.name} -> {schema_category}/sessions/")
                
                except Exception as e:
                    print(f"   âŒ å¤„ç†ä¼šè¯å¤±è´¥ {session_dir.name}: {e}")
    
    # 3. ç”Ÿæˆåˆ†ç±»ç»Ÿè®¡æŠ¥å‘Š
    print(f"\nğŸ“Š ç”Ÿæˆåˆ†ç±»ç»Ÿè®¡æŠ¥å‘Š:")
    
    classification_stats = {}
    
    for schema_key, schema_info in schema_structure.items():
        schema_dir = results_path / "outputs" / "by_schema" / schema_key
        
        # ç»Ÿè®¡å„ç±»æ–‡ä»¶æ•°é‡
        kg_count = len(list((schema_dir / "knowledge_graphs").glob("*.json")))
        session_count = len(list((schema_dir / "sessions").iterdir()))
        
        classification_stats[schema_key] = {
            "name": schema_info["name"],
            "knowledge_graphs": kg_count,
            "sessions": session_count,
            "description": schema_info["description"]
        }
        
        print(f"   ğŸ“‹ {schema_info['name']}: {kg_count} ä¸ªKG, {session_count} ä¸ªä¼šè¯")
    
    # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
    stats_file = results_path / "outputs" / "schema_classification_report.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump({
            "classification_time": datetime.now().isoformat(),
            "total_schemas": len(schema_structure),
            "statistics": classification_stats,
            "file_naming_convention": {
                "knowledge_graphs": "{session_id}_{schema_type}_knowledge_graph.json",
                "analysis": "{session_id}_{schema_type}_analysis.json",
                "exports": "{session_id}_{schema_type}_{format}.{ext}"
            }
        }, f, ensure_ascii=False, indent=2)
    
    print(f"   ğŸ’¾ ç»Ÿè®¡æŠ¥å‘Šä¿å­˜åˆ°: {stats_file}")
    
    # 4. åˆ›å»ºè¾¹ç•Œè¯´æ˜æ–‡æ¡£
    boundary_doc = results_path / "SCHEMA_BOUNDARIES.md"
    boundary_content = """# Schemaè¾¹ç•Œå’Œè¾“å‡ºè§„èŒƒ

## ğŸ¯ æ ¸å¿ƒåŸåˆ™
**è¾“å…¥æ··ä¹±ï¼Œè¾“å‡ºè§„èŒƒ** - æ— è®ºè¾“å…¥æ–‡æ¡£å¤šä¹ˆæ··ä¹±ï¼Œè¾“å‡ºå¿…é¡»ä¸¥æ ¼ç¬¦åˆå¯¹åº”Schema

## ğŸ“‹ Schemaåˆ†ç±»è¾¹ç•Œ

### 1. é€šç”¨çŸ¥è¯†å›¾è°± (general)
- **é€‚ç”¨åœºæ™¯**: ä¼ä¸šæ¶æ„ã€ç³»ç»Ÿæ¡†æ¶ã€ç®—æ³•æ¨¡å‹
- **æ ¸å¿ƒå®ä½“**: Framework, Algorithm, Paradigm, Task, Technique
- **æ ¸å¿ƒå…³ç³»**: is_instance_of, implements, uses_paradigm, has_algorithm
- **è¾“å‡ºè¦æ±‚**: æ‰€æœ‰å®ä½“å¿…é¡»æ˜¯æ¶æ„æˆ–æŠ€æœ¯æ¦‚å¿µ

### 2. æ—¶ç©ºçŸ¥è¯†å›¾è°± (spatiotemporal)  
- **é€‚ç”¨åœºæ™¯**: æ—¶ç©ºäº‹ä»¶ã€DO-DA-Fç»“æ„ã€æ°´åˆ©ç³»ç»Ÿ
- **æ ¸å¿ƒå®ä½“**: TemporalEntity, SpatialEntity, Event, Action, Condition, Outcome
- **æ ¸å¿ƒå…³ç³»**: hasStartTime, locatedAt, hasCondition, resultsIn
- **è¾“å‡ºè¦æ±‚**: å¿…é¡»åŒ…å«æ—¶é—´æˆ–ç©ºé—´ç»´åº¦

### 3. æ··åˆç±»å‹ (mixed)
- **é€‚ç”¨åœºæ™¯**: æ— æ³•æ˜ç¡®åˆ†ç±»æˆ–åŒ…å«å¤šç§Schemaå…ƒç´ 
- **å¤„ç†æ–¹å¼**: äººå·¥å®¡æ ¸åé‡æ–°åˆ†ç±»
- **è¾“å‡ºè¦æ±‚**: æ ‡æ³¨æ··åˆåŸå› å’Œå»ºè®®åˆ†ç±»

## ğŸ”§ è´¨é‡æ§åˆ¶æ£€æŸ¥ç‚¹

### æ£€æŸ¥ç‚¹1: Schemaä¸€è‡´æ€§
- [ ] æ‰€æœ‰å®ä½“ç±»å‹åœ¨Schemaä¸­å®šä¹‰
- [ ] æ‰€æœ‰å…³ç³»ç±»å‹åœ¨Schemaä¸­å®šä¹‰  
- [ ] å®ä½“-å…³ç³»ç»„åˆç¬¦åˆSchemaçº¦æŸ

### æ£€æŸ¥ç‚¹2: è¯­ä¹‰æ­£ç¡®æ€§
- [ ] å®ä½“åç§°è¯­ä¹‰åˆç†
- [ ] å…³ç³»æ–¹å‘æ­£ç¡®
- [ ] ä¸‰å…ƒç»„é€»è¾‘ä¸€è‡´

### æ£€æŸ¥ç‚¹3: å®Œæ•´æ€§
- [ ] é‡è¦æ¦‚å¿µæœªé—æ¼
- [ ] å…³ç³»ç½‘ç»œè¿é€š
- [ ] å…ƒæ•°æ®å®Œæ•´

## ğŸ“ æ–‡ä»¶å‘½åè§„èŒƒ

```
{session_id}_{schema_type}_{file_type}.{ext}

ç¤ºä¾‹:
- 20250816_110515_general_knowledge_graph.json
- 20250816_110516_spatiotemporal_analysis.json
- 20250816_110517_mixed_export.csv
```

## ğŸš¨ è¾¹ç•Œè¿è§„å¤„ç†

1. **å®ä½“ç±»å‹ä¸åŒ¹é…**: è‡ªåŠ¨é‡æ–°æ¨æ–­æˆ–æ ‡è®°ä¸ºUnknown
2. **å…³ç³»ç±»å‹ä¸å­˜åœ¨**: æ˜ å°„åˆ°æœ€ç›¸è¿‘çš„åˆæ³•å…³ç³»æˆ–ä¸¢å¼ƒ
3. **Schemaæ··åˆ**: è‡ªåŠ¨åˆ†ç±»åˆ°mixedç±»åˆ«ï¼Œç­‰å¾…äººå·¥å®¡æ ¸

## ğŸ“Š è´¨é‡æŒ‡æ ‡

- **Schemaä¸€è‡´æ€§**: 100% (å¼ºåˆ¶è¦æ±‚)
- **å®ä½“æ¨æ–­å‡†ç¡®ç‡**: >80%
- **å…³ç³»æŠ½å–å‡†ç¡®ç‡**: >75%
- **å¤„ç†é€Ÿåº¦**: <5s/æ–‡æ¡£
"""
    
    with open(boundary_doc, 'w', encoding='utf-8') as f:
        f.write(boundary_content)
    
    print(f"   ğŸ“ è¾¹ç•Œè¯´æ˜æ–‡æ¡£: {boundary_doc}")
    
    print(f"\nâœ… Schemaåˆ†ç±»é‡ç»„å®Œæˆ!")
    return classification_stats

def classify_schema(schema_path: str) -> str:
    """æ ¹æ®schemaè·¯å¾„åˆ†ç±»"""
    if not schema_path:
        return "mixed"
    
    schema_path_lower = schema_path.lower()
    
    if "schema_config" in schema_path_lower or "general" in schema_path_lower:
        return "general"
    elif "spatiotemporal" in schema_path_lower or "spatial" in schema_path_lower:
        return "spatiotemporal"
    else:
        return "mixed"

def extract_session_id(filename: str) -> str:
    """ä»æ–‡ä»¶åæå–ä¼šè¯ID"""
    # å°è¯•æå–æ—¶é—´æˆ³æ ¼å¼çš„ä¼šè¯ID
    parts = filename.split('_')
    if len(parts) >= 2 and parts[0].isdigit() and parts[1].isdigit():
        return f"{parts[0]}_{parts[1]}"
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨æ–‡ä»¶åå‰ç¼€
    return filename.split('.')[0]

if __name__ == "__main__":
    reorganize_by_schema()
