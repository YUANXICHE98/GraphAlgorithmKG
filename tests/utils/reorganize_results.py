#!/usr/bin/env python3
"""
é‡æ–°æ•´ç†resultsæ–‡ä»¶ç»“æž„
åˆ›å»ºæ¸…æ™°ã€æœ‰åºçš„æ–‡ä»¶ç»„ç»‡
"""

import os
import shutil
import json
from pathlib import Path
from datetime import datetime

def reorganize_results():
    """é‡æ–°æ•´ç†resultsç›®å½•ç»“æž„"""
    
    print("ðŸ§¹ å¼€å§‹é‡æ–°æ•´ç†resultsæ–‡ä»¶ç»“æž„")
    print("=" * 60)
    
    results_path = Path("results")
    
    # 1. åˆ›å»ºæ–°çš„æ¸…æ™°ç»“æž„
    new_structure = {
        "sessions": "å¤„ç†ä¼šè¯çš„å®Œæ•´è®°å½•",
        "outputs": {
            "knowledge_graphs": "æœ€ç»ˆçš„çŸ¥è¯†å›¾è°±æ–‡ä»¶",
            "analysis": "åˆ†æžæŠ¥å‘Š",
            "exports": "å¯¼å‡ºçš„å„ç§æ ¼å¼æ–‡ä»¶"
        },
        "cache": {
            "entities": "å®žä½“æŽ¨æ–­ç¼“å­˜",
            "schemas": "Schemaæ£€æµ‹ç¼“å­˜"
        },
        "backups": "åŽ†å²å¤‡ä»½æ–‡ä»¶"
    }
    
    print("ðŸ“ åˆ›å»ºæ–°çš„ç›®å½•ç»“æž„:")
    
    # åˆ›å»ºç›®å½•
    for key, value in new_structure.items():
        if isinstance(value, dict):
            for subkey, subdesc in value.items():
                dir_path = results_path / key / subkey
                dir_path.mkdir(parents=True, exist_ok=True)
                print(f"   âœ… {key}/{subkey}: {subdesc}")
        else:
            dir_path = results_path / key
            dir_path.mkdir(parents=True, exist_ok=True)
            print(f"   âœ… {key}: {value}")
    
    # 2. ç§»åŠ¨å’Œæ•´ç†çŽ°æœ‰æ–‡ä»¶
    print(f"\nðŸ“¦ æ•´ç†çŽ°æœ‰æ–‡ä»¶:")
    
    # ç§»åŠ¨currentç›®å½•ä¸‹çš„æ–‡ä»¶åˆ°exports
    current_path = results_path / "current"
    if current_path.exists():
        exports_path = results_path / "outputs" / "exports"
        
        # ç§»åŠ¨å„ç§æ ¼å¼çš„å¯¼å‡ºæ–‡ä»¶
        export_files = [
            "entities.csv", "entities.json",
            "relations.csv", "relations.json", 
            "edges.txt", "statistics.json",
            "knowledge_graph.gexf", "knowledge_graph.graphml"
        ]
        
        for file_name in export_files:
            src_file = current_path / file_name
            if src_file.exists():
                dst_file = exports_path / file_name
                shutil.move(str(src_file), str(dst_file))
                print(f"   ðŸ“„ ç§»åŠ¨: {file_name} -> outputs/exports/")
        
        # ç§»åŠ¨å¤‡ä»½æ–‡ä»¶
        backups_src = current_path / "backups"
        backups_dst = results_path / "backups"
        if backups_src.exists():
            if backups_dst.exists():
                # åˆå¹¶å¤‡ä»½æ–‡ä»¶
                for backup_file in backups_src.glob("*.pkl"):
                    dst_file = backups_dst / backup_file.name
                    if not dst_file.exists():
                        shutil.move(str(backup_file), str(dst_file))
                        print(f"   ðŸ’¾ ç§»åŠ¨å¤‡ä»½: {backup_file.name}")
                shutil.rmtree(backups_src)
            else:
                shutil.move(str(backups_src), str(backups_dst))
                print(f"   ðŸ’¾ ç§»åŠ¨æ•´ä¸ªå¤‡ä»½ç›®å½•")
        
        # ç§»åŠ¨å¯è§†åŒ–æ–‡ä»¶
        viz_src = current_path / "visualizations"
        viz_dst = exports_path / "visualizations"
        if viz_src.exists():
            shutil.move(str(viz_src), str(viz_dst))
            print(f"   ðŸŽ¨ ç§»åŠ¨å¯è§†åŒ–æ–‡ä»¶")
        
        # åˆ é™¤ç©ºçš„currentç›®å½•
        if current_path.exists() and not any(current_path.iterdir()):
            current_path.rmdir()
            print(f"   ðŸ—‘ï¸ åˆ é™¤ç©ºçš„currentç›®å½•")
    
    # 3. æ¸…ç†ç©ºçš„ä¼šè¯ç›®å½•
    sessions_path = results_path / "sessions"
    if sessions_path.exists():
        empty_sessions = []
        valid_sessions = []
        
        for session_dir in sessions_path.iterdir():
            if session_dir.is_dir() and session_dir.name != "latest":
                if not any(session_dir.iterdir()):
                    empty_sessions.append(session_dir.name)
                    session_dir.rmdir()
                else:
                    valid_sessions.append(session_dir.name)
        
        print(f"   ðŸ—‘ï¸ åˆ é™¤ç©ºä¼šè¯ç›®å½•: {len(empty_sessions)} ä¸ª")
        print(f"   âœ… ä¿ç•™æœ‰æ•ˆä¼šè¯: {len(valid_sessions)} ä¸ª")
        
        for session in valid_sessions:
            print(f"      - {session}")
    
    # 4. æ£€æŸ¥outputsç›®å½•ä¸­çš„æ–‡ä»¶
    outputs_kg_path = results_path / "outputs" / "knowledge_graphs"
    if outputs_kg_path.exists():
        kg_files = list(outputs_kg_path.glob("*.json"))
        print(f"   ðŸ“Š çŸ¥è¯†å›¾è°±æ–‡ä»¶: {len(kg_files)} ä¸ª")
        
        for kg_file in kg_files:
            file_size = kg_file.stat().st_size / 1024  # KB
            print(f"      - {kg_file.name}: {file_size:.1f} KB")
    
    outputs_analysis_path = results_path / "outputs" / "analysis"
    if outputs_analysis_path.exists():
        analysis_files = list(outputs_analysis_path.glob("*.json"))
        print(f"   ðŸ“ˆ åˆ†æžæ–‡ä»¶: {len(analysis_files)} ä¸ª")
        
        for analysis_file in analysis_files:
            file_size = analysis_file.stat().st_size / 1024  # KB
            print(f"      - {analysis_file.name}: {file_size:.1f} KB")
    
    # 5. åˆ›å»ºREADMEæ–‡ä»¶è¯´æ˜Žç»“æž„
    readme_content = """# Results ç›®å½•ç»“æž„è¯´æ˜Ž

## ðŸ“ ç›®å½•ç»“æž„

```
results/
â”œâ”€â”€ sessions/           # å¤„ç†ä¼šè¯è®°å½•
â”‚   â”œâ”€â”€ YYYYMMDD_HHMMSS/   # æŒ‰æ—¶é—´æˆ³å‘½åçš„ä¼šè¯ç›®å½•
â”‚   â”‚   â”œâ”€â”€ 01_document_input.json      # æ–‡æ¡£è¾“å…¥
â”‚   â”‚   â”œâ”€â”€ 02_schema_detection.json    # Schemaæ£€æµ‹ç»“æžœ
â”‚   â”‚   â”œâ”€â”€ 03_triple_extraction.json   # ä¸‰å…ƒç»„æŠ½å–ç»“æžœ
â”‚   â”‚   â”œâ”€â”€ 04_entity_inference.json    # å®žä½“æŽ¨æ–­ç»“æžœ
â”‚   â”‚   â”œâ”€â”€ 05_final_kg.json           # æœ€ç»ˆçŸ¥è¯†å›¾è°±
â”‚   â”‚   â””â”€â”€ session_metadata.json      # ä¼šè¯å…ƒæ•°æ®
â”‚   â””â”€â”€ latest -> YYYYMMDD_HHMMSS/     # æŒ‡å‘æœ€æ–°ä¼šè¯çš„è½¯é“¾æŽ¥
â”œâ”€â”€ outputs/            # æœ€ç»ˆè¾“å‡ºæ–‡ä»¶
â”‚   â”œâ”€â”€ knowledge_graphs/   # çŸ¥è¯†å›¾è°±JSONæ–‡ä»¶
â”‚   â”œâ”€â”€ analysis/          # åˆ†æžæŠ¥å‘Š
â”‚   â””â”€â”€ exports/           # å„ç§æ ¼å¼çš„å¯¼å‡ºæ–‡ä»¶
â”‚       â”œâ”€â”€ *.csv          # CSVæ ¼å¼
â”‚       â”œâ”€â”€ *.json         # JSONæ ¼å¼
â”‚       â”œâ”€â”€ *.gexf         # Gephiæ ¼å¼
â”‚       â”œâ”€â”€ *.graphml      # GraphMLæ ¼å¼
â”‚       â””â”€â”€ visualizations/ # å¯è§†åŒ–æ–‡ä»¶
â”œâ”€â”€ cache/              # ç¼“å­˜æ•°æ®
â”‚   â”œâ”€â”€ entities/       # å®žä½“æŽ¨æ–­ç¼“å­˜
â”‚   â””â”€â”€ schemas/        # Schemaæ£€æµ‹ç¼“å­˜
â””â”€â”€ backups/            # åŽ†å²å¤‡ä»½æ–‡ä»¶
    â””â”€â”€ *.pkl           # åºåˆ—åŒ–å¤‡ä»½
```

## ðŸ“‹ æ–‡ä»¶è¯´æ˜Ž

### ä¼šè¯æ–‡ä»¶ (sessions/)
- æ¯ä¸ªä¼šè¯åŒ…å«å®Œæ•´çš„å¤„ç†æµç¨‹è®°å½•
- æ–‡ä»¶æŒ‰å¤„ç†é¡ºåºç¼–å· (01, 02, 03...)
- å¯ä»¥è¿½æº¯ä»»ä½•é˜¶æ®µçš„ä¸­é—´ç»“æžœ

### è¾“å‡ºæ–‡ä»¶ (outputs/)
- `knowledge_graphs/`: æœ€ç»ˆçš„çŸ¥è¯†å›¾è°±JSONæ–‡ä»¶
- `analysis/`: å¤„ç†åˆ†æžå’Œç»Ÿè®¡æŠ¥å‘Š
- `exports/`: å„ç§æ ¼å¼çš„å¯¼å‡ºæ–‡ä»¶ï¼Œä¾¿äºŽå…¶ä»–å·¥å…·ä½¿ç”¨

### ç¼“å­˜æ–‡ä»¶ (cache/)
- æé«˜é‡å¤å¤„ç†çš„æ•ˆçŽ‡
- å®žä½“æŽ¨æ–­å’ŒSchemaæ£€æµ‹çš„ç¼“å­˜ç»“æžœ

### å¤‡ä»½æ–‡ä»¶ (backups/)
- åŽ†å²ç‰ˆæœ¬çš„åºåˆ—åŒ–å¤‡ä»½
- ç”¨äºŽæ¢å¤å’Œç‰ˆæœ¬å¯¹æ¯”
"""
    
    readme_path = results_path / "README.md"
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"\nðŸ“ åˆ›å»ºREADME.mdè¯´æ˜Žæ–‡ä»¶")
    
    # 6. ç”Ÿæˆç»“æž„æŠ¥å‘Š
    print(f"\nðŸ“Š æœ€ç»ˆæ–‡ä»¶ç»“æž„æŠ¥å‘Š:")
    print_directory_tree(results_path)
    
    print(f"\nâœ… æ–‡ä»¶ç»“æž„æ•´ç†å®Œæˆ!")

def print_directory_tree(path, prefix="", max_depth=3, current_depth=0):
    """æ‰“å°ç›®å½•æ ‘ç»“æž„"""
    if current_depth >= max_depth:
        return
    
    path = Path(path)
    if not path.exists():
        return
    
    items = sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name))
    
    for i, item in enumerate(items):
        is_last = i == len(items) - 1
        current_prefix = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
        
        if item.is_file():
            size = item.stat().st_size / 1024  # KB
            print(f"{prefix}{current_prefix}{item.name} ({size:.1f} KB)")
        else:
            print(f"{prefix}{current_prefix}{item.name}/")
            
            if current_depth < max_depth - 1:
                next_prefix = prefix + ("    " if is_last else "â”‚   ")
                print_directory_tree(item, next_prefix, max_depth, current_depth + 1)

if __name__ == "__main__":
    reorganize_results()
