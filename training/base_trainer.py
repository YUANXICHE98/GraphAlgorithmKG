"""
è®­ç»ƒæ¨¡å‹åŸºç¡€æ¡†æ¶
æä¾›ç»Ÿä¸€çš„è®­ç»ƒæ¥å£å’Œæ¨¡å‹ç®¡ç†
"""

import os
import json
import pickle
import numpy as np
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    model_name: str
    model_type: str  # schema_classifier, entity_inferer, relation_extractor, entity_merger
    training_data_path: str
    validation_data_path: Optional[str]
    output_model_path: str
    hyperparameters: Dict[str, Any]
    training_metadata: Dict[str, Any]

@dataclass
class TrainingResult:
    """è®­ç»ƒç»“æœ"""
    model_path: str
    training_time: float
    final_accuracy: float
    validation_accuracy: Optional[float]
    training_history: List[Dict[str, float]]
    model_metadata: Dict[str, Any]

class BaseTrainer(ABC):
    """è®­ç»ƒå™¨åŸºç±»"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.model = None
        self.training_history = []
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(config.output_model_path).parent.mkdir(parents=True, exist_ok=True)
    
    @abstractmethod
    def prepare_data(self) -> Tuple[Any, Any, Any, Any]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        Returns: (X_train, y_train, X_val, y_val)
        """
        pass
    
    @abstractmethod
    def build_model(self) -> Any:
        """æ„å»ºæ¨¡å‹"""
        pass
    
    @abstractmethod
    def train_model(self, X_train: Any, y_train: Any, X_val: Any, y_val: Any) -> TrainingResult:
        """è®­ç»ƒæ¨¡å‹"""
        pass
    
    @abstractmethod
    def evaluate_model(self, X_test: Any, y_test: Any) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        pass
    
    def save_model(self, model_path: str = None) -> str:
        """ä¿å­˜æ¨¡å‹"""
        if model_path is None:
            model_path = self.config.output_model_path
        
        # ä¿å­˜æ¨¡å‹
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'model_name': self.config.model_name,
            'model_type': self.config.model_type,
            'training_time': datetime.now().isoformat(),
            'hyperparameters': self.config.hyperparameters,
            'training_history': self.training_history,
            'config': self.config.__dict__
        }
        
        metadata_path = model_path.replace('.pkl', '_metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        print(f"ğŸ“‹ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
        
        return model_path
    
    def load_model(self, model_path: str) -> Any:
        """åŠ è½½æ¨¡å‹"""
        with open(model_path, 'rb') as f:
            self.model = pickle.load(f)
        
        # åŠ è½½å…ƒæ•°æ®
        metadata_path = model_path.replace('.pkl', '_metadata.json')
        if os.path.exists(metadata_path):
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                self.training_history = metadata.get('training_history', [])
        
        print(f"ğŸ“‚ æ¨¡å‹å·²åŠ è½½: {model_path}")
        return self.model
    
    def run_full_training(self) -> TrainingResult:
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹: {self.config.model_name}")
        print(f"ğŸ“‹ æ¨¡å‹ç±»å‹: {self.config.model_type}")
        print("=" * 80)
        
        start_time = datetime.now()
        
        # 1. å‡†å¤‡æ•°æ®
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        X_train, y_train, X_val, y_val = self.prepare_data()
        train_size = X_train.shape[0] if hasattr(X_train, 'shape') else len(X_train) if X_train is not None else 0
        val_size = X_val.shape[0] if hasattr(X_val, 'shape') else len(X_val) if X_val is not None else 0
        print(f"   è®­ç»ƒé›†å¤§å°: {train_size}")
        print(f"   éªŒè¯é›†å¤§å°: {val_size}")
        
        # 2. æ„å»ºæ¨¡å‹
        print("ğŸ—ï¸ æ„å»ºæ¨¡å‹...")
        self.model = self.build_model()
        
        # 3. è®­ç»ƒæ¨¡å‹
        print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
        result = self.train_model(X_train, y_train, X_val, y_val)
        
        # 4. ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        model_path = self.save_model()
        result.model_path = model_path
        
        training_time = (datetime.now() - start_time).total_seconds()
        result.training_time = training_time
        
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"   â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.2f}s")
        print(f"   ğŸ“ˆ æœ€ç»ˆå‡†ç¡®ç‡: {result.final_accuracy:.4f}")
        if result.validation_accuracy:
            print(f"   ğŸ“Š éªŒè¯å‡†ç¡®ç‡: {result.validation_accuracy:.4f}")
        print("=" * 80)
        
        return result

class TrainingDataManager:
    """è®­ç»ƒæ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "training/data/raw"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    def collect_schema_training_data(self) -> str:
        """æ”¶é›†Schemaåˆ†ç±»è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š æ”¶é›†Schemaåˆ†ç±»è®­ç»ƒæ•°æ®...")
        
        # ä»ä¼šè¯è®°å½•ä¸­æ”¶é›†æ•°æ®
        sessions_dir = Path("results/sessions")
        training_data = []
        
        if sessions_dir.exists():
            for session_dir in sessions_dir.iterdir():
                if session_dir.is_dir() and session_dir.name != "latest":
                    try:
                        # è¯»å–æ–‡æ¡£è¾“å…¥
                        doc_input_file = session_dir / "01_document_input.json"
                        schema_detection_file = session_dir / "02_schema_detection.json"
                        
                        if doc_input_file.exists() and schema_detection_file.exists():
                            with open(doc_input_file, 'r', encoding='utf-8') as f:
                                doc_data = json.load(f)
                            
                            with open(schema_detection_file, 'r', encoding='utf-8') as f:
                                schema_data = json.load(f)
                            
                            # æ„å»ºè®­ç»ƒæ ·æœ¬
                            sample = {
                                'text': doc_data['data']['content'],
                                'schema': schema_data['data']['selected_schema'],
                                'session_id': session_dir.name
                            }
                            training_data.append(sample)
                    
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†ä¼šè¯ {session_dir.name} å¤±è´¥: {e}")
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        output_file = self.data_dir / "schema_classification_data.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ”¶é›†åˆ° {len(training_data)} ä¸ªSchemaåˆ†ç±»æ ·æœ¬")
        print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
        
        return str(output_file)
    
    def collect_entity_training_data(self) -> str:
        """æ”¶é›†å®ä½“æ¨æ–­è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š æ”¶é›†å®ä½“æ¨æ–­è®­ç»ƒæ•°æ®...")
        
        sessions_dir = Path("results/sessions")
        training_data = []
        
        if sessions_dir.exists():
            for session_dir in sessions_dir.iterdir():
                if session_dir.is_dir() and session_dir.name != "latest":
                    try:
                        # è¯»å–å®ä½“æ¨æ–­ç»“æœ
                        entity_file = session_dir / "04_entity_inference.json"
                        
                        if entity_file.exists():
                            with open(entity_file, 'r', encoding='utf-8') as f:
                                entity_data = json.load(f)
                            
                            # æå–å®ä½“è®­ç»ƒæ ·æœ¬
                            for entity in entity_data['data']['entities']:
                                sample = {
                                    'entity_name': entity['name'],
                                    'entity_type': entity['type'],
                                    'confidence': entity['confidence'],
                                    'schema': entity['schema'],
                                    'properties': entity['properties'],
                                    'session_id': session_dir.name
                                }
                                training_data.append(sample)
                    
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†ä¼šè¯ {session_dir.name} å¤±è´¥: {e}")
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        output_file = self.data_dir / "entity_inference_data.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ”¶é›†åˆ° {len(training_data)} ä¸ªå®ä½“æ¨æ–­æ ·æœ¬")
        print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
        
        return str(output_file)
    
    def collect_relation_training_data(self) -> str:
        """æ”¶é›†å…³ç³»æŠ½å–è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š æ”¶é›†å…³ç³»æŠ½å–è®­ç»ƒæ•°æ®...")
        
        sessions_dir = Path("results/sessions")
        training_data = []
        
        if sessions_dir.exists():
            for session_dir in sessions_dir.iterdir():
                if session_dir.is_dir() and session_dir.name != "latest":
                    try:
                        # è¯»å–ä¸‰å…ƒç»„æŠ½å–ç»“æœ
                        triple_file = session_dir / "03_triple_extraction.json"
                        
                        if triple_file.exists():
                            with open(triple_file, 'r', encoding='utf-8') as f:
                                triple_data = json.load(f)
                            
                            # æå–å…³ç³»è®­ç»ƒæ ·æœ¬
                            for triple in triple_data['data']['triples']:
                                sample = {
                                    'subject': triple['subject'],
                                    'predicate': triple['predicate'],
                                    'object': triple['object'],
                                    'confidence': triple.get('confidence', 0.8),
                                    'method': triple.get('method', 'unknown'),
                                    'session_id': session_dir.name
                                }
                                training_data.append(sample)
                    
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†ä¼šè¯ {session_dir.name} å¤±è´¥: {e}")
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        output_file = self.data_dir / "relation_extraction_data.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ”¶é›†åˆ° {len(training_data)} ä¸ªå…³ç³»æŠ½å–æ ·æœ¬")
        print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
        
        return str(output_file)

# å…¨å±€è®­ç»ƒæ•°æ®ç®¡ç†å™¨
training_data_manager = TrainingDataManager()
