"""
å®ä½“æ¨æ–­æ¨¡å‹è®­ç»ƒå™¨
ç”¨äºè®­ç»ƒæ›´å‡†ç¡®çš„å®ä½“ç±»å‹æ¨æ–­æ¨¡å‹
"""

import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from typing import Dict, List, Any, Optional, Tuple
import re

from .base_trainer import BaseTrainer, TrainingConfig, TrainingResult

class EntityInfererTrainer(BaseTrainer):
    """å®ä½“æ¨æ–­å™¨è®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        super().__init__(config)
        self.vectorizer = None
        self.label_encoder = {}
        self.reverse_label_encoder = {}
        self.feature_extractors = {}
    
    def extract_features(self, entity_name: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æå–å®ä½“ç‰¹å¾"""
        features = {}
        
        # åŸºç¡€æ–‡æœ¬ç‰¹å¾
        features['length'] = len(entity_name)
        features['word_count'] = len(entity_name.split())
        features['has_uppercase'] = any(c.isupper() for c in entity_name)
        features['has_digits'] = any(c.isdigit() for c in entity_name)
        features['has_special_chars'] = bool(re.search(r'[^a-zA-Z0-9\s]', entity_name))
        
        # è¯æ±‡ç‰¹å¾
        name_lower = entity_name.lower()
        features['starts_with_capital'] = entity_name[0].isupper() if entity_name else False
        features['is_all_caps'] = entity_name.isupper()
        features['contains_id'] = 'id' in name_lower or '_' in name_lower
        
        # é¢†åŸŸç‰¹å®šå…³é”®è¯
        temporal_keywords = ['time', 'date', 'æ—¶é—´', 'æ—¥æœŸ', 'å°æ—¶', 'åˆ†é’Ÿ', 'ç§’', 'hour', 'minute', 'second']
        spatial_keywords = ['location', 'place', 'position', 'ä½ç½®', 'åœ°ç‚¹', 'ç©ºé—´', 'space', 'area']
        action_keywords = ['action', 'execute', 'perform', 'æ‰§è¡Œ', 'åŠ¨ä½œ', 'æ“ä½œ', 'monitor', 'check']
        condition_keywords = ['condition', 'threshold', 'limit', 'æ¡ä»¶', 'é˜ˆå€¼', 'é™åˆ¶', 'status', 'state']
        
        features['is_temporal'] = any(kw in name_lower for kw in temporal_keywords)
        features['is_spatial'] = any(kw in name_lower for kw in spatial_keywords)
        features['is_action'] = any(kw in name_lower for kw in action_keywords)
        features['is_condition'] = any(kw in name_lower for kw in condition_keywords)
        
        # æ¨¡å¼åŒ¹é…
        features['is_camel_case'] = bool(re.search(r'[a-z][A-Z]', entity_name))
        features['is_snake_case'] = '_' in entity_name
        features['is_id_pattern'] = bool(re.search(r'ID_\d+|id_\d+|\d+', entity_name))
        features['is_event_pattern'] = entity_name.endswith('Event') or 'äº‹ä»¶' in entity_name
        
        # ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆå¦‚æœæä¾›ï¼‰
        if context:
            features['schema_type'] = context.get('schema', 'unknown')
            features['confidence'] = context.get('confidence', 0.5)
        
        return features
    
    def prepare_data(self) -> Tuple[Any, Any, Any, Any]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š åŠ è½½å®ä½“æ¨æ–­è®­ç»ƒæ•°æ®...")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        with open(self.config.training_data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        features_list = []
        labels = []
        entity_names = []
        
        for sample in data:
            entity_name = sample['entity_name']
            entity_type = sample['entity_type']
            
            # è·³è¿‡Unknownç±»å‹ï¼ˆå™ªå£°æ•°æ®ï¼‰
            if entity_type == 'Unknown':
                continue
            
            # æå–ç‰¹å¾
            context = {
                'schema': sample.get('schema', 'unknown'),
                'confidence': sample.get('confidence', 0.5)
            }
            features = self.extract_features(entity_name, context)
            
            features_list.append(features)
            labels.append(entity_type)
            entity_names.append(entity_name)
        
        print(f"   ğŸ“ å®ä½“æ ·æœ¬: {len(features_list)}")
        print(f"   ğŸ·ï¸ æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(*np.unique(labels, return_counts=True)))}")
        
        # åˆ›å»ºæ ‡ç­¾ç¼–ç å™¨
        unique_labels = list(set(labels))
        self.label_encoder = {label: idx for idx, label in enumerate(unique_labels)}
        self.reverse_label_encoder = {idx: label for label, idx in self.label_encoder.items()}
        
        # ç¼–ç æ ‡ç­¾
        encoded_labels = [self.label_encoder[label] for label in labels]
        
        # è½¬æ¢ç‰¹å¾ä¸ºæ•°å€¼çŸ©é˜µ
        feature_names = list(features_list[0].keys())
        X = []
        
        for features in features_list:
            feature_vector = []
            for name in feature_names:
                value = features[name]
                if isinstance(value, bool):
                    feature_vector.append(1.0 if value else 0.0)
                elif isinstance(value, (int, float)):
                    feature_vector.append(float(value))
                elif isinstance(value, str):
                    # ç®€å•çš„å­—ç¬¦ä¸²ç¼–ç 
                    feature_vector.append(hash(value) % 1000 / 1000.0)
                else:
                    feature_vector.append(0.0)
            X.append(feature_vector)
        
        X = np.array(X)
        y = np.array(encoded_labels)
        
        # ä¿å­˜ç‰¹å¾åç§°
        self.feature_names = feature_names
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        test_size = self.config.hyperparameters.get('test_size', 0.2)
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        print(f"   ğŸ¯ è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬, {X_train.shape[1]} ç‰¹å¾")
        print(f"   ğŸ“Š éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬")
        
        return X_train, y_train, X_val, y_val
    
    def build_model(self) -> GradientBoostingClassifier:
        """æ„å»ºæ¢¯åº¦æå‡åˆ†ç±»å™¨"""
        print("ğŸš€ æ„å»ºæ¢¯åº¦æå‡åˆ†ç±»å™¨...")
        
        model = GradientBoostingClassifier(
            n_estimators=self.config.hyperparameters.get('n_estimators', 100),
            learning_rate=self.config.hyperparameters.get('learning_rate', 0.1),
            max_depth=self.config.hyperparameters.get('max_depth', 6),
            min_samples_split=self.config.hyperparameters.get('min_samples_split', 10),
            min_samples_leaf=self.config.hyperparameters.get('min_samples_leaf', 4),
            random_state=42
        )
        
        return model
    
    def train_model(self, X_train: Any, y_train: Any, X_val: Any, y_val: Any) -> TrainingResult:
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
        
        # è®­ç»ƒæ¨¡å‹
        self.model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_train_pred = self.model.predict(X_train)
        y_val_pred = self.model.predict(X_val)
        
        # è®¡ç®—å‡†ç¡®ç‡
        train_accuracy = accuracy_score(y_train, y_train_pred)
        val_accuracy = accuracy_score(y_val, y_val_pred)
        
        print(f"   ğŸ“ˆ è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
        print(f"   ğŸ“Š éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
        target_names = [self.reverse_label_encoder[i] for i in range(len(self.reverse_label_encoder))]
        print(classification_report(y_val, y_val_pred, target_names=target_names))
        
        # ç‰¹å¾é‡è¦æ€§
        importances = self.model.feature_importances_
        top_features = sorted(zip(self.feature_names, importances), key=lambda x: x[1], reverse=True)
        
        print("\nğŸ” é‡è¦ç‰¹å¾ (Top 15):")
        for feature, importance in top_features[:15]:
            print(f"   {feature}: {importance:.4f}")
        
        # æ„å»ºè®­ç»ƒç»“æœ
        result = TrainingResult(
            model_path="",  # å°†åœ¨ä¿å­˜æ—¶è®¾ç½®
            training_time=0.0,  # å°†åœ¨å¤–éƒ¨è®¾ç½®
            final_accuracy=train_accuracy,
            validation_accuracy=val_accuracy,
            training_history=[{
                'epoch': 1,
                'train_accuracy': train_accuracy,
                'val_accuracy': val_accuracy
            }],
            model_metadata={
                'model_type': 'GradientBoostingClassifier',
                'feature_count': X_train.shape[1],
                'feature_names': self.feature_names,
                'label_encoder': self.label_encoder,
                'reverse_label_encoder': self.reverse_label_encoder,
                'top_features': top_features[:10]
            }
        )
        
        return result
    
    def evaluate_model(self, X_test: Any, y_test: Any) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        return {
            'accuracy': accuracy,
            'sample_count': len(y_test)
        }
    
    def predict_entity_type(self, entity_name: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """é¢„æµ‹å®ä½“ç±»å‹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒæˆ–æœªåŠ è½½")
        
        # æå–ç‰¹å¾
        features = self.extract_features(entity_name, context)
        
        # è½¬æ¢ä¸ºç‰¹å¾å‘é‡
        feature_vector = []
        for name in self.feature_names:
            value = features.get(name, 0)
            if isinstance(value, bool):
                feature_vector.append(1.0 if value else 0.0)
            elif isinstance(value, (int, float)):
                feature_vector.append(float(value))
            elif isinstance(value, str):
                feature_vector.append(hash(value) % 1000 / 1000.0)
            else:
                feature_vector.append(0.0)
        
        X = np.array([feature_vector])
        
        # é¢„æµ‹
        pred_proba = self.model.predict_proba(X)[0]
        pred_label_idx = np.argmax(pred_proba)
        pred_label = self.reverse_label_encoder[pred_label_idx]
        confidence = pred_proba[pred_label_idx]
        
        # æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        all_probabilities = {
            self.reverse_label_encoder[i]: prob 
            for i, prob in enumerate(pred_proba)
        }
        
        return {
            'predicted_type': pred_label,
            'confidence': confidence,
            'all_probabilities': all_probabilities,
            'features_used': features
        }

def create_entity_inferer_config(
    training_data_path: str,
    output_model_path: str = "training/models/entity_inferer/latest.pkl",
    hyperparameters: Dict[str, Any] = None
) -> TrainingConfig:
    """åˆ›å»ºå®ä½“æ¨æ–­å™¨è®­ç»ƒé…ç½®"""
    
    if hyperparameters is None:
        hyperparameters = {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'min_samples_split': 10,
            'min_samples_leaf': 4,
            'test_size': 0.2
        }
    
    return TrainingConfig(
        model_name="å®ä½“ç±»å‹æ¨æ–­å™¨",
        model_type="entity_inferer",
        training_data_path=training_data_path,
        validation_data_path=None,
        output_model_path=output_model_path,
        hyperparameters=hyperparameters,
        training_metadata={
            'description': 'åŸºäºç‰¹å¾å·¥ç¨‹å’Œæ¢¯åº¦æå‡çš„å®ä½“ç±»å‹æ¨æ–­å™¨',
            'input_format': 'entity_name + context',
            'output_format': 'entity_type',
            'feature_types': ['textual', 'lexical', 'pattern', 'domain_specific']
        }
    )

def train_entity_inferer(
    training_data_path: str = None,
    output_model_path: str = "training/models/entity_inferer/latest.pkl",
    hyperparameters: Dict[str, Any] = None
) -> TrainingResult:
    """è®­ç»ƒå®ä½“æ¨æ–­å™¨çš„ä¾¿æ·å‡½æ•°"""
    
    # å¦‚æœæ²¡æœ‰æä¾›è®­ç»ƒæ•°æ®è·¯å¾„ï¼Œè‡ªåŠ¨æ”¶é›†
    if training_data_path is None:
        from .base_trainer import training_data_manager
        training_data_path = training_data_manager.collect_entity_training_data()
    
    # åˆ›å»ºé…ç½®
    config = create_entity_inferer_config(
        training_data_path=training_data_path,
        output_model_path=output_model_path,
        hyperparameters=hyperparameters
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ
    trainer = EntityInfererTrainer(config)
    result = trainer.run_full_training()
    
    return result

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šè®­ç»ƒå®ä½“æ¨æ–­å™¨
    result = train_entity_inferer()
    print(f"ğŸ‰ å®ä½“æ¨æ–­å™¨è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {result.model_path}")
    print(f"ğŸ“ˆ éªŒè¯å‡†ç¡®ç‡: {result.validation_accuracy:.4f}")
